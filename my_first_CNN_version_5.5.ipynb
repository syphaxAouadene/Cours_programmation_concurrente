{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkQ6rm96K4nkX1mZd/uK0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syphaxAouadene/Cours_programmation_concurrente/blob/main/my_first_CNN_version_5.5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8cv5FlwWprm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJVQMAlSsyCc",
        "outputId": "a0cf6a1f-fbb2-4bd9-b5df-0039821fc422"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYSvMPYsy05"
      },
      "source": [
        "from mlxtend.data import loadlocal_mnist\n",
        "import platform"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg09QpqgtTza",
        "outputId": "52ce2016-f3d6-4808-c166-9cc47e46779b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zLyEq5leMs8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXOxpXATeNlp"
      },
      "source": [
        "# CNN operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChRRlpbQasLv"
      },
      "source": [
        "def multiplication(imaget, f):\n",
        "    if len(imaget.shape) == 1 :\n",
        "        imaget = imaget.reshape((imaget.shape[0], 1))\n",
        "    result = imaget * f\n",
        "    somme = 0\n",
        "    for i in range(imaget.shape[0]):\n",
        "        for j in range(imaget.shape[1]):\n",
        "            somme = somme + result[i, j]\n",
        "    return somme\n",
        "\n",
        "from scipy import signal\n",
        "def convolution(img, f):\n",
        "    return signal.convolve2d(img, f, mode='valid')\n",
        "# def convolution(img, f):\n",
        "#     result = np.zeros((img.shape[0] - f.shape[0] + 1, img.shape[1] - f.shape[1] + 1))\n",
        "#     for i in range(result.shape[0]):\n",
        "#         for j in range(result.shape[1]):\n",
        "#             imaget = img[i:f.shape[0]+i, j:f.shape[1]+j]\n",
        "#             multi = multiplication(imaget, f)\n",
        "#             result[i, j] = multi\n",
        "#     return result\n",
        "\n",
        "\n",
        "def ReLU_convolution(convolved_map):\n",
        "    result = np.zeros(convolved_map.shape)\n",
        "    for i in range(result.shape[0]):\n",
        "        for j in range(result.shape[1]):\n",
        "            result[i, j] = np.max([convolved_map[i,j], 0])\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_convolved_layer_from_previous_layer(previous_layer, nbr_filter, size_filter):\n",
        "    \"\"\"\n",
        "    don't forget to add bias term corresponding to each filter !\n",
        "    - previous_layer = is a list that contains each feature map of the previous_layer\n",
        "    - nbr_filter = is an integer that represents how many filter do we want to use, (ex. 6)\n",
        "    - size_filter = is an integer that represents the shape of each filter (if size_filter=3 then shape_filter=(3, 3))\n",
        "    \n",
        "    this function return a list that contains each convolved map \n",
        "    (you should know that convolved_map = convolution between feature_map and filter)\n",
        "    \"\"\"\n",
        "    convolved_layer = []\n",
        "    # filters = [initialize_filter(size_filter, size_filter) for i in range(nbr_filter)]\n",
        "    # biais = initialize_filter(nbr_filter, 1)\n",
        "    for f in filters:\n",
        "        somme = 0\n",
        "        bias = 0\n",
        "        for feature_map in previous_layer:\n",
        "            somme = somme + convolution(feature_map, f)\n",
        "        somme = somme + bias\n",
        "        convolved_layer.append(ReLU_convolution(somme))\n",
        "    return convolved_layer, filters, biais\n",
        "\n",
        "\n",
        "def max_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "#     result_of_pooling has to have shape = ((input_width - kernel_width + 2*padding)/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.max(imaget)\n",
        "    return result\n",
        "\n",
        "\n",
        "def mean_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.mean(imaget)\n",
        "    return result\n",
        "    \n",
        "    \n",
        "def min_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.min(imaget)\n",
        "    return result\n",
        "\n",
        "\n",
        "def pooling(convolved_layer, type_of_pooling, size_of_pooling_kernel, stride):\n",
        "    \"\"\"\n",
        "    convolved_layer : is a list that contains each convolved_map from previous_layer\n",
        "    type_of_pooling : should be either 'MAX_POOLING' or 'MEAN_POOLING' or 'MIN_POOLING'\n",
        "    size_of_pooling_kernel : is an integer that represents the shape of kernel \n",
        "                            (if size_of_pooling_kernel=2 then shape_kernel=(2, 2))\n",
        "    this function return a list that contains each pooled_map\n",
        "    \"\"\"\n",
        "    pooled_layer = []\n",
        "    switcher = {\n",
        "        'MAX_POOLING': max_pooling,\n",
        "        'MEAN_POOLING': mean_pooling,\n",
        "        'MIN_POOLING': min_pooling\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    pooling_operation = switcher.get(type_of_pooling, lambda: \"Invalid type_of_pooling !\")\n",
        "    # Execute the function\n",
        "    for convolved_map in convolved_layer:\n",
        "        pooled_layer.append(pooling_operation(convolved_map, size_of_pooling_kernel, stride))\n",
        "    return pooled_layer\n",
        "\n",
        "\n",
        "def initialize_filter(filter_width, filter_height):\n",
        "    \"\"\"\n",
        "    cette fonction s'occupe de l'initialisation d'un filtre aléatoirement selon la distribution normale\n",
        "    \"\"\"\n",
        "    return np.random.randn(filter_width, filter_height)\n",
        "\n",
        "\n",
        "# def show_image(img):\n",
        "#     plt.imshow(img, cmap=plt.cm.binary)\n",
        "#     plt.show()\n",
        "    \n",
        "    \n",
        "def show_multiple_images(images, nbr_of_images=5):\n",
        "    for img in images[:nbr_of_images]:\n",
        "        show_image(img)\n",
        "        time.sleep(1)\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        \n",
        "# def flatten(layer):\n",
        "#     \"\"\"\n",
        "#     #arguments:\n",
        "#     layer : is a list of feature_maps\n",
        "#     #returns : list of all numbers that contained in the feature maps in the layer\n",
        "#     \"\"\"\n",
        "#     result = []\n",
        "#     for matrix in layer:\n",
        "#         result = result + list(matrix.flatten())\n",
        "#     result = np.array(result).reshape((1, len(result)))\n",
        "#     return result\n",
        "\n",
        "# def fully_connected_layer(input_layer, nbr_neurons, activation_function='ReLU'):\n",
        "#     current_layer = []\n",
        "#     input_layer = np.array(input_layer)\n",
        "#     for i in range(nbr_neurons):\n",
        "#         bias = 0\n",
        "#         weights = np.random.randn(len(input_layer), 1)\n",
        "#         current_layer.append(np.max([multiplication(input_layer, weights) + bias, 0]))\n",
        "#     return current_layer\n",
        "\n",
        "\n",
        "# def softmax(data):\n",
        "#     output = []\n",
        "#     for value in data:\n",
        "#         proba_value = np.exp(value)/(np.sum(np.exp(data)))\n",
        "#         output.append(proba_value)\n",
        "#     return output\n",
        "        \n",
        "# def categoricalCrossEntropy(generated_values, target_values):\n",
        "#     somme = 0\n",
        "#     for i in range(len(generated_values)):\n",
        "#         somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "#     return (-1) * somme  \n",
        "\n",
        "#######################################################################################\n",
        "#######  Fully_connected_layer_functions\n",
        "#######################################################################################\n",
        "\n",
        "# def update_weights(dL_dY, weights, inputs, lrate):\n",
        "#     \"\"\"\n",
        "#     arguments :\n",
        "#     dL_dY : un vecteur des dérivées de la couche supérieure par rapport a la couche de sortie Y de dimension n_outputs\n",
        "#     weights : la matrice des poids de dimension (n_inputs x n_outputs)\n",
        "#     inputs : le vecteur de sortie de la couche précedente de dimension n_inputs\n",
        "#     lrate : learning rate (scalar)\n",
        "#     \"\"\"\n",
        "#     dL_dW = []\n",
        "#     for xi in inputs:\n",
        "#         dL_dW = dL_dW + xi * dL_dY\n",
        "#     new_weights = flatten(weights) - lrate * dL_dW\n",
        "#     new_weights = new_weights.reshape(weights.shape)\n",
        "#     return new_weights\n",
        "\n",
        "# def calcul_dL_dX(dL_dY, weights):\n",
        "#     return np.dot(dL_dY, np.transpose(weights))\n",
        "\n",
        "\n",
        "# def fcl(inputs_layer, nbr_neurons, weights, biais, activation_type='ReLU'):\n",
        "#     current_layer = []\n",
        "#     current_layer = flatten(np.dot(inputs_layer, weights) + biais)[0]\n",
        "#     output_layer = activation_function(current_layer, activation_type)\n",
        "#     return output_layer\n",
        "   \n",
        "# def activation_function(layer, type_of_activation='relu'):\n",
        "#     type_of_activation = type_of_activation.lower()\n",
        "#     switcher = {\n",
        "#         'relu': ReLU,\n",
        "#         'tanh': tanh,\n",
        "#         'segmoid': segmoid\n",
        "#     }\n",
        "#     # Get the function from switcher dictionary\n",
        "#     activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' !\")\n",
        "#     return activation_type(layer)\n",
        "    \n",
        "    \n",
        "# def ReLU(layer):\n",
        "#     layer = np.array(layer)\n",
        "#     result = []\n",
        "#     for y in layer:\n",
        "#         result.append(np.max([y, 0]))\n",
        "#     return result\n",
        "\n",
        "# def tanh(layer):\n",
        "#     layer = np.array(layer)\n",
        "#     result = []\n",
        "#     for y in layer:\n",
        "#         r = (np.exp(y)-np.exp(-1*y))/(np.exp(y)+np.exp(-1*y))\n",
        "#         result.append(r)\n",
        "#     return result\n",
        "\n",
        "# def segmoid(layer):\n",
        "#     layer = np.array(layer)\n",
        "#     result = []\n",
        "#     for y in layer:\n",
        "#         r = 1/(1+np.exp(-1*y))\n",
        "#         result.append(r)\n",
        "#     return result\n",
        "\n",
        "def back_error_from_end_to_output_fcl(y_hat, y):\n",
        "    \"\"\"\n",
        "    arguments : y_hat = list des outputs calculés par le forward, et y = list des targets\n",
        "    cette fonction va calculer l'erreur de son origine(end_of_network) jusqu'à le output de fully_connected_layer\n",
        "    soit ce bout de network :\n",
        "    X ---> softmax(X) ---> CCE(y_hat, y) ---> Loss\n",
        "    alors cette fonction va retourner la dérivée de l'erreur Loss par rapport à X\n",
        "    càd elle return dL_dX\n",
        "    \"\"\"\n",
        "    return y_hat - y\n",
        "\n",
        "\n",
        "def unflatten(vector, pooled_layer):\n",
        "    pooled_layer = np.array(pooled_layer)\n",
        "    vector = np.array(vector)\n",
        "    # print(len(vector))\n",
        "    vector = vector.reshape(pooled_layer.shape)\n",
        "    return vector\n",
        "        \n",
        "        "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JGBU6bUeWwd"
      },
      "source": [
        "# Fully Connected Layer Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lin2XPzwsy7J"
      },
      "source": [
        "def activation_function(layer, type_of_activation='relu'):\n",
        "    type_of_activation = type_of_activation.lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' !\")\n",
        "    return activation_type(layer)\n",
        "    \n",
        "\n",
        "def ReLU(layer):\n",
        "    return layer * (layer > 0)\n",
        "\n",
        "\n",
        "def d_ReLU(layer):\n",
        "    return 1. * (layer > 0)\n",
        "\n",
        "\n",
        "def tanh(layer):\n",
        "    print('tanh')\n",
        "    r = (np.exp(layer)-np.exp(-1*layer))/(np.exp(layer)+np.exp(-1*layer))   \n",
        "    return np.array(r)\n",
        "\n",
        "\n",
        "def d_tanh(layer):\n",
        "    return 1 - tanh(layer) * tanh(layer)\n",
        "\n",
        "\n",
        "def segmoid(layer):\n",
        "    # print('segmoid')\n",
        "    return np.array(1/(1+np.exp(-1*layer)))\n",
        "\n",
        "\n",
        "def d_segmoid(vector):\n",
        "    \"\"\"\n",
        "    cette fontion prend un vector en entrée et retourne la dérivée de segmoid par rapport a ce vector\n",
        "    \"\"\"\n",
        "    return segmoid(vector) * (1 - segmoid(vector))\n",
        "\n",
        "\n",
        "# def softmax(data):\n",
        "#     # print('softmax')\n",
        "#     # print(data)\n",
        "#     proba_values = np.exp(data)/(np.sum(np.exp(data)))   \n",
        "#     return np.array(proba_values)\n",
        "\n",
        "def softmax(x):\n",
        "    maxi = np.max(x)\n",
        "    return np.exp(x-maxi)/sum(np.exp(x-maxi))\n",
        "\n",
        "\n",
        "def categoricalCrossEntropy(generated_values, target_values):\n",
        "    generated_values = [[1.0e-100] if r[0]==0.0 else r for r in generated_values]\n",
        "    somme = 0\n",
        "    for i in range(len(generated_values)):\n",
        "        somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "    return (-1) * somme \n",
        "\n",
        "\n",
        "def normelize(img):\n",
        "    return img/255 - 0.5\n",
        "\n",
        "\n",
        "def flatten(img):\n",
        "    img = np.array(img) \n",
        "    return img.flatten()\n",
        "\n",
        "\n",
        "def show_image(img):\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def init_params(my_network):\n",
        "    nbr_layers = len(my_network) - 1\n",
        "    W, B = [], []\n",
        "    for i in range(nbr_layers):\n",
        "        W.append(np.random.randn(my_network[i+1], my_network[i]))\n",
        "        B.append(np.random.randn(my_network[i+1], 1))\n",
        "    return W, B\n",
        "\n",
        "\n",
        "def forward_pass(img, W, B):\n",
        "    \"\"\"\n",
        "    here we will use this notation :\n",
        "    Z[i] = W[i].X + B[i]\n",
        "    A[i] = activation_function(Z[i])\n",
        "    Z is a list that carries all the output of each layer\n",
        "    A is a list that carries all the output of each activation function\n",
        "    \"\"\"\n",
        "    act_functions = activation_functions_fcl[1:-1] # we omit the first element and the last one because the first activation will be None, and the last one will always be 'softmax'\n",
        "    act_functions = [type_of_activation.lower() for type_of_activation in act_functions] # lawercase all the items\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_types = [switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function\") for type_of_activation in act_functions]\n",
        "    Z, A = [], [img]\n",
        "    for i in range(len(W)):\n",
        "        if i == len(W)-1: # we have to use softmax as activation layer because we're in the last layer\n",
        "            Z.append(np.dot(W[i], A[i]) + B[i])\n",
        "            A.append(softmax(Z[i]))\n",
        "        else: # we're in hidden layer\n",
        "            Z.append(np.dot(W[i], A[i]) + B[i])\n",
        "            A.append(activation_types[i](Z[i]))\n",
        "    return Z, A\n",
        "\n",
        "\n",
        "def one_hot(y):\n",
        "    return np.eye(10)[y].reshape(10, 1)\n",
        "\n",
        "\n",
        "def update_W_and_B(W, dL_dW, B, dL_dB, lr):\n",
        "    \"\"\"\n",
        "    this function update the weights and Biais of myNetwork\n",
        "    arguments : \n",
        "    - W : it is a list that contains each Weight vector ([W1, W2, ...])\n",
        "    - dL_dW : derivatives of loss with respect to Weights (it is a list that contains Weights derivatives vectors [dL_dW1, dL_dW2, ...])\n",
        "    - B : it is a list that contains each Biais vector ([B1, B2, ...])\n",
        "    - dL_dB : derivatives of loss with respect to Biais (it is a list that contains Biais derivatives vectors [dL_dB1, dL_dB2, ...])\n",
        "    - lr : learning rate (real number)\n",
        "    \"\"\"\n",
        "    # print('Update_W_and_B')\n",
        "    new_W = []\n",
        "    new_B = []\n",
        "    \n",
        "    for w, dw in zip(W, dL_dW):\n",
        "        try:\n",
        "            w = np.array(w) - lr * np.array(dw)\n",
        "        except:\n",
        "            w = None\n",
        "        \n",
        "        new_W.append(w)\n",
        "    for b, db in zip(B, dL_dB):\n",
        "        try:\n",
        "            b = np.array(b) - lr * np.array(db)\n",
        "        except:\n",
        "            b = None\n",
        "        \n",
        "        new_B.append(b)\n",
        "        \n",
        "    return new_W, new_B\n",
        "\n",
        "\n",
        "def compute_accuracy(my_cnn, x_val, y_val, W, B):\n",
        "    '''\n",
        "        This function does a forward pass of x_validation, then checks if the indices\n",
        "        of the maximum value in the output equals the indices in the label\n",
        "        y. Then it sums over each prediction and calculates the accuracy.\n",
        "    '''\n",
        "    predictions = []\n",
        "\n",
        "    for x, y in zip(x_val, y_val):\n",
        "        # prepare the input image\n",
        "        # X = flatten(x)\n",
        "        # X = X.reshape(len(X), 1)\n",
        "        Y = one_hot(y)\n",
        "        # forward-propagation\n",
        "        Z, A = forward_propagation([x], my_cnn, W, B)\n",
        "        output = A[-1]\n",
        "        pred = np.argmax(output)\n",
        "        predictions.append(pred == np.argmax(Y))\n",
        "\n",
        "    return np.mean(predictions)\n",
        "\n",
        "\n",
        "def classify(img, W, B):\n",
        "    \"\"\"\n",
        "    cette fonction recois une seule image en parametre\n",
        "    et elle reçois les poids W et les Biais B et la liste des fonctions d'activations\n",
        "    et elle retourne la catégorie de l'image en entier 0..9\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    X = flatten(img)\n",
        "    X = X.reshape(len(X), 1)\n",
        "    # forward-propagation\n",
        "    Z, A = forward_pass(X, W, B)\n",
        "    output = A[-1]\n",
        "    pred = np.argmax(output)\n",
        "    return pred\n",
        "\n",
        "\n",
        "def show_accuracies(my_cnn, train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B):\n",
        "    \"\"\"\n",
        "    this function compute accuracy for each train-set, validation-set, and test-set\n",
        "    then print them all.\n",
        "    arguments : train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B\n",
        "    \"\"\"\n",
        "    train_accuracy = compute_accuracy(my_cnn, train_images, train_labels, W, B)\n",
        "    val_accuracy = compute_accuracy(my_cnn, val_images, val_labels, W, B)\n",
        "    test_accuracy = compute_accuracy(my_cnn, test_images, test_labels, W, B)\n",
        "    print(\"Accuracies :\\n\\\n",
        "    - train accuracy = {} %\\n\\\n",
        "    - val accuracy = {} %\\n\\\n",
        "    - test accuracy = {} %\".format(train_accuracy*100, val_accuracy*100, test_accuracy*100))\n",
        "\n",
        "\n",
        "def backpro_pass(dL_dZ, A, Z, W, indice, indx_act_func):\n",
        "    # we omit the first element and the last one because the first activation will be None, and the last one will always be 'softmax'\n",
        "    act_functions = activation_functions[1:-1] \n",
        "    # lawercase all the items\n",
        "    act_functions = [type_of_activation.lower() for type_of_activation in act_functions] \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_types = [switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function\") for type_of_activation in act_functions]\n",
        "    \n",
        "    dl_dw = np.dot(dL_dZ, np.transpose(A[indice]))\n",
        "    dl_db = dL_dZ\n",
        "    dl_dz = 0\n",
        "    if indice*(-1) != len(Z)+1:\n",
        "      dl_da = np.dot(np.transpose(W[indice+1]), dL_dZ)\n",
        "      da_dz = activation_types[indx_act_func](Z[indice])\n",
        "      dl_dz = dl_da * da_dz\n",
        "    return dl_dw, dl_db, dl_dz"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx6wKd0xuzaj"
      },
      "source": [
        "# Here we will upload the dataset and normelize it then shuffle it then split it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQfMoEksy91"
      },
      "source": [
        "images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-images.idx3-ubyte'\n",
        "labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-labels.idx1-ubyte'\n",
        "test_images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-images.idx3-ubyte'\n",
        "test_labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-labels.idx1-ubyte'\n",
        "test_images, test_labels = loadlocal_mnist(test_images_path, test_labels_path)\n",
        "train_images, train_labels = loadlocal_mnist(images_path, labels_path)\n",
        "\n",
        "# group all the images in one list\n",
        "# then normelize all the images\n",
        "images = np.concatenate([train_images, test_images])\n",
        "labels = np.concatenate([train_labels, test_labels])\n",
        "images = normelize(images)\n",
        "\n",
        "# shuffle all the images and all labels randomly\n",
        "random.seed(102)\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "labels = labels[indices]\n",
        "images = images[indices]\n",
        "\n",
        "# change shape of the images\n",
        "images = images.reshape(len(images), 28, 28)\n",
        "\n",
        "# split the data into train, validation and test \n",
        "train_images, val_images, test_images = images[:60000], images[60000:65000], images[65000:]\n",
        "train_labels, val_labels, test_labels = labels[:60000], labels[60000:65000], labels[65000:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3BP2WmGnLq7"
      },
      "source": [
        "# Learning  ------>  GO FOR LAUNCH !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFh0nVttXh36"
      },
      "source": [
        "def input_layer(dict):\n",
        "  dict['type_of_layer'] = 'input'\n",
        "  return dict\n",
        "\n",
        "def convolution_layer(dict):\n",
        "  dict['type_of_layer'] = 'convolution'\n",
        "  return dict\n",
        "\n",
        "def pooling_layer(dict):\n",
        "  dict['type_of_layer'] = 'pooling'\n",
        "  return dict\n",
        "\n",
        "def flatten_layer():\n",
        "  dict = {'type_of_layer': 'flatten'}\n",
        "  return dict\n",
        "\n",
        "def fcl(dict):\n",
        "  dict['type_of_layer'] = 'fcl'\n",
        "  return dict"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ltquYZzS18U",
        "outputId": "72d1ab87-9c21-4456-f0bc-c62088107374"
      },
      "source": [
        "\n",
        "my_cnn = [input_layer({\n",
        "              'width_image': 28,\n",
        "              'height_image': 28,\n",
        "              'nbr_channels': 1   # 1 --> means gray scale, and 3 --> means rgb\n",
        "              }\n",
        "          ),\n",
        "          convolution_layer({\n",
        "              'nbr_of_kernels':6, \n",
        "              'kernel_size':3, \n",
        "              'padding':0, \n",
        "              'stride':1, \n",
        "              'type_of_activation':'relu'\n",
        "              }\n",
        "          ), \n",
        "          pooling_layer({\n",
        "              'type_of_pooling' : 'MAX_POOLING',\n",
        "              'kernel_size' : 2,\n",
        "              'stride' : 2\n",
        "              } \n",
        "          ),\n",
        "          # convolution_layer({\n",
        "          #     'nbr_of_kernels':6, \n",
        "          #     'kernel_size':5, \n",
        "          #     'padding':0, \n",
        "          #     'stride':1, \n",
        "          #     'type_of_activation':'relu'\n",
        "          #     }\n",
        "          # ), \n",
        "          # pooling_layer({\n",
        "          #     'type_of_pooling' : 'MAX_POOLING',\n",
        "          #     'kernel_size' : 5,\n",
        "          #     'stride' : 2\n",
        "          #     } \n",
        "          # ),\n",
        "          flatten_layer(),\n",
        "        #   fcl({\n",
        "        #       'nbr_of_neurons' : 128, # 20 neurons in hidden layer\n",
        "        #       'type_of_activation' : 'segmoid', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "        #       'learning_rate' : 0.001\n",
        "        #         }\n",
        "        #   ),\n",
        "          fcl({\n",
        "              'nbr_of_neurons' : 10, # nbr of neurons in output_layer layer\n",
        "              'type_of_activation' : 'softmax', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "              'learning_rate' : 0.001\n",
        "          }\n",
        "          )\n",
        "          ]\n",
        "my_cnn"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'height_image': 28,\n",
              "  'nbr_channels': 1,\n",
              "  'type_of_layer': 'input',\n",
              "  'width_image': 28},\n",
              " {'kernel_size': 3,\n",
              "  'nbr_of_kernels': 6,\n",
              "  'padding': 0,\n",
              "  'stride': 1,\n",
              "  'type_of_activation': 'relu',\n",
              "  'type_of_layer': 'convolution'},\n",
              " {'kernel_size': 2,\n",
              "  'stride': 2,\n",
              "  'type_of_layer': 'pooling',\n",
              "  'type_of_pooling': 'MAX_POOLING'},\n",
              " {'type_of_layer': 'flatten'},\n",
              " {'learning_rate': 0.001,\n",
              "  'nbr_of_neurons': 10,\n",
              "  'type_of_activation': 'softmax',\n",
              "  'type_of_layer': 'fcl'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTrefR2JcCJ9"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeKni3_lcCNO"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LcSNc8EcCXI"
      },
      "source": [
        "def forward_propagation(img, my_cnn, W, B):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'convolution': convolution_operation,\n",
        "        'pooling': pooling_operation,\n",
        "        'flatten': flatten_operation,\n",
        "        'fcl' : fcl_operation\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_activation_function\") for type_of_layer in my_cnn_architecture]\n",
        "    Z, A = [img], [img]\n",
        "    for i in range(1, len(my_cnn)):\n",
        "        z, a = operation_types[i](my_cnn[i], A, W, B, i)\n",
        "        Z.append(z)\n",
        "        A.append(a)\n",
        "    return Z, A"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA-ZtPajnUMc"
      },
      "source": [
        "# def input_operation(layer, A, W, B, layer_num):\n",
        "#     a = None\n",
        "#     z = None\n",
        "#     return a, z\n",
        "\n",
        "def convolution_operation(layer, A, W, B, layer_num):\n",
        "    previous_layer = A[-1]\n",
        "    nbr_filters = layer['nbr_of_kernels']\n",
        "    size_filter = layer['kernel_size']\n",
        "    convolved_layer = []\n",
        "    filters = W[layer_num]\n",
        "    biais = B[layer_num]\n",
        "    z = []\n",
        "    for i in range(len(filters)):\n",
        "        somme = 0\n",
        "        for feature_map in previous_layer:\n",
        "            somme = somme + convolution(feature_map, filters[i])\n",
        "        somme = somme + biais[i]\n",
        "        z.append(somme)\n",
        "        convolved_layer.append(ReLU(somme))\n",
        "    return z, convolved_layer\n",
        "\n",
        "def pooling_operation(layer, A, W, B, layer_num):\n",
        "    \"\"\"\n",
        "    convolved_layer : is a list that contains each convolved_map from previous_layer\n",
        "    type_of_pooling : should be either 'MAX_POOLING' or 'MEAN_POOLING' or 'MIN_POOLING'\n",
        "    size_of_pooling_kernel : is an integer that represents the shape of kernel \n",
        "                            (if size_of_pooling_kernel=2 then shape_kernel=(2, 2))\n",
        "    this function return a list that contains each pooled_map\n",
        "    \"\"\"\n",
        "    previous_layer = A[-1]\n",
        "    type_of_pooling = layer['type_of_pooling']\n",
        "    size_of_pooling_kernel = layer['kernel_size']\n",
        "    stride = layer['stride']\n",
        "    pooled_layer = []\n",
        "    switcher = {\n",
        "        'MAX_POOLING': max_pooling,\n",
        "        'MEAN_POOLING': mean_pooling,\n",
        "        'MIN_POOLING': min_pooling\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    pooling_operation = switcher.get(type_of_pooling, lambda: \"Invalid type_of_pooling !\")\n",
        "    # Execute the function\n",
        "    for map in previous_layer:\n",
        "        pooled_layer.append(pooling_operation(map, size_of_pooling_kernel, stride))\n",
        "    return pooled_layer, pooled_layer\n",
        "\n",
        "\n",
        "def flatten_operation(layer, A, W, B, layer_num):\n",
        "    a = flatten(A[-1])\n",
        "    a = a.reshape(len(a), 1)   \n",
        "    return a, a\n",
        "\n",
        "def fcl_operation(layer, A, Weights, B, layer_num):\n",
        "    # print('fcl_operation')\n",
        "    global W\n",
        "    global compteur\n",
        "    compteur += 1\n",
        "    input_fcl = A[-1]\n",
        "    if compteur == 1:\n",
        "        weights_fcl = np.random.randn(layer['nbr_of_neurons'], len(input_fcl))\n",
        "        W[layer_num] = weights_fcl\n",
        "    else:\n",
        "        weights_fcl = W[layer_num]\n",
        "    biais_fcl = B[layer_num]\n",
        "    type_of_activation = layer['type_of_activation'].lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid,\n",
        "        'softmax': softmax\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' or 'softmax' !\")\n",
        "    \n",
        "    # print(input_fcl)\n",
        "    # print(biais_fcl)\n",
        "    output_fcl = np.dot(weights_fcl, input_fcl) + biais_fcl\n",
        "    output = activation_type(output_fcl)\n",
        "    return output_fcl, output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0sy1r9Toqt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enj51gJnZ6fO"
      },
      "source": [
        "def input_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "def convolution_init_W_and_B(my_cnn, num_layer):\n",
        "    nbr_filters = my_cnn[num_layer]['nbr_of_kernels']\n",
        "    size_kernel = my_cnn[num_layer]['kernel_size']\n",
        "    w = [initialize_filter(size_kernel, size_kernel) for i in range(nbr_filters)]\n",
        "    b = initialize_filter(nbr_filters, 1)\n",
        "    return np.array(w), np.array(b)\n",
        "\n",
        "def pooling_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def flatten_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "def fcl_init_W_and_B(my_cnn, num_layer):\n",
        "    global compteur_init\n",
        "    nbr_neurons = my_cnn[num_layer]['nbr_of_neurons']\n",
        "    if compteur_init == 0:\n",
        "        w = None\n",
        "        compteur_init += 1\n",
        "    else : \n",
        "        nbr_neurons_previous_layer = my_cnn[num_layer - 1]['nbr_of_neurons']\n",
        "        w = np.random.randn(nbr_neurons, nbr_neurons_previous_layer)\n",
        "    \n",
        "    b = initialize_filter(nbr_neurons, 1)\n",
        "    return w, np.array(b)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64fCBYvWYorg"
      },
      "source": [
        "def initialization(my_cnn):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'input': input_init_W_and_B,\n",
        "        'convolution': convolution_init_W_and_B,\n",
        "        'pooling': pooling_init_W_and_B,\n",
        "        'flatten': flatten_init_W_and_B,\n",
        "        'fcl' : fcl_init_W_and_B\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_activation_function\") for type_of_layer in my_cnn_architecture]\n",
        "    W, B = [], []\n",
        "    for i in range(len(my_cnn)):\n",
        "        w, b = operation_types[i](my_cnn, i)\n",
        "        # print(my_cnn[i])\n",
        "        # print(w)\n",
        "        W.append(w)\n",
        "        B.append(b)\n",
        "    \n",
        "    return np.array(W), np.array(B)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjTvf748dlgp"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HsM2r104MRj"
      },
      "source": [
        "def backpro_input(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print(\"backpro_input\")\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "def backpro_convolution(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print(\"backpro_convolution\")\n",
        "    dl_dz = d_ReLU(np.array(Z[num_layer])) * dL_dZ[-1]\n",
        "    dL_dF = []\n",
        "    dL_dX = []\n",
        "    \n",
        "    dL_dB = [np.sum(t) for t in dl_dz]\n",
        "    for map in A[num_layer - 1]:\n",
        "        X = np.array(map)\n",
        "        for dz in dl_dz:\n",
        "            dL_dF.append(convolution(X, dz))\n",
        "        # dl_dx = full_convolution(np.array(W[num_layer]), dL_dZ[-1])\n",
        "        # dL_dX.append(dl_dx)\n",
        "    # for dl_dz in dL_dZ[-1]:\n",
        "    #     dl_db = np.sum(dl_dz)\n",
        "    #     dL_dB.append(dl_db)\n",
        "    dL_dB = np.array(dL_dB)\n",
        "    dL_dB = dL_dB.reshape((dL_dB.shape[0], 1))\n",
        "    return dL_dF, dL_dB, dL_dX\n",
        "\n",
        "def backpro_pooling(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print(\"backpro_pooling\")\n",
        "    dl_dz = []\n",
        "    result = np.array(dL_dZ[-1])\n",
        "    stride = my_cnn[num_layer]['stride']\n",
        "    size_of_pooling_kernel = my_cnn[num_layer]['kernel_size']\n",
        "    for map, convolved_map in zip(result, Z[num_layer - 1]):\n",
        "        dl_dz_map = np.zeros_like(convolved_map)\n",
        "        for i in range(0, map.shape[0]):\n",
        "            for j in range(0, map.shape[1]):\n",
        "                value = map[i][j]\n",
        "                imaget = convolved_map[i*stride:i*stride+size_of_pooling_kernel, j*stride:j*stride+size_of_pooling_kernel]\n",
        "                imaget_max_index = np.argmax(imaget)\n",
        "                # imaget_max_value = np.max(imaget)\n",
        "                ligne, colonne = imaget_max_index//size_of_pooling_kernel , imaget_max_index%size_of_pooling_kernel\n",
        "                imaget = np.zeros_like(imaget)\n",
        "                imaget[ligne][colonne] = value\n",
        "                dl_dz_map[i*stride:i*stride+size_of_pooling_kernel, j*stride:j*stride+size_of_pooling_kernel] = imaget\n",
        "        dl_dz.append(np.array(dl_dz_map))\n",
        "\n",
        "    return None, None, np.array(dl_dz)\n",
        "\n",
        "def backpro_flatten(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print('backpro_flatten')\n",
        "    # print(type(dL_dZ[-1]))\n",
        "    # print(np.array(A[num_layer-1]).shape)\n",
        "    dl_dz = unflatten(np.array(dL_dZ[-1]), A[num_layer-1])\n",
        "    return None, None, dl_dz\n",
        "\n",
        "def backpro_fcl(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print('backpro_fcl')\n",
        "    type_of_activation = my_cnn[num_layer-1].get('type_of_activation')\n",
        "    \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    if type_of_activation != None:\n",
        "        activation_type = switcher.get(type_of_activation, lambda: None)\n",
        "\n",
        "    # print(dL_dZ)\n",
        "    # print(num_layer)\n",
        "    dl_dw = np.dot(dL_dZ[-1], np.transpose(A[num_layer - 1]))\n",
        "    dl_db = dL_dZ[-1]\n",
        "    dl_da = np.dot(np.transpose(W[num_layer]), dL_dZ[-1])\n",
        "    \n",
        "    if type_of_activation == None:\n",
        "        dl_dz = dl_da\n",
        "    else:\n",
        "        da_dz = activation_type(np.array(Z[num_layer - 1]))\n",
        "        dl_dz = dl_da * da_dz\n",
        "    return np.array(dl_dw), np.array(dl_db), np.array(dl_dz)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_PuOYDcCeH"
      },
      "source": [
        "def backpropagation(my_cnn, dL_dZ, W, Z, A):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "\n",
        "    switcher = {\n",
        "        'convolution': backpro_convolution,\n",
        "        'pooling': backpro_pooling,\n",
        "        'flatten': backpro_flatten,\n",
        "        'fcl' : backpro_fcl,\n",
        "        'input' : backpro_input\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_activation_function\") for type_of_layer in my_cnn_architecture]\n",
        "    dL_dW, dL_dB = [], []\n",
        "    \n",
        "    for num_layer in range(len(my_cnn)-1, -1, -1): # iterate through all layers from output to input\n",
        "        dl_dw, dl_db, dl_dz = operation_types[num_layer](my_cnn, dL_dZ, W, Z, A, num_layer)\n",
        "        # print(num_layer, operation_types[num_layer], dl_dw)\n",
        "        dL_dW.append(dl_dw)\n",
        "        dL_dB.append(dl_db)\n",
        "        dL_dZ.append(dl_dz)\n",
        "    return dL_dW, dL_dB\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTHjzsuI9WXC"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "nofPg2oxcCZC",
        "outputId": "7ec1aef5-0e85-4d3e-fd12-5ffb80e4aa17"
      },
      "source": [
        "np.random.seed(577)\n",
        "numbers_of_epochs = 100\n",
        "compteur_init = 0\n",
        "W, B = initialization(my_cnn)\n",
        "compteur = 0\n",
        "all_losses = []\n",
        "for epoch in range(numbers_of_epochs):\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    for i in range(len(train_images)):\n",
        "    # for i in range(10000):\n",
        "        # Z, A = [train_images[i]], [train_images[i]]\n",
        "        # Prepare the input image\n",
        "        # X = flatten(train_images[i])\n",
        "        # X = X.reshape(len(X), 1)\n",
        "        Y = one_hot(train_labels[i])\n",
        "        # if i==0 or i==5 or i==10 or i==15 or i==17 or i==18:\n",
        "        #     print(i, '-----> \\n',W)\n",
        "        # print('------------------------------------------------------------------------------------------')\n",
        "        Z, A = forward_propagation([train_images[i]], my_cnn, W, B)\n",
        "        loss = categoricalCrossEntropy(A[-1], Y)\n",
        "        # print(A[-1])\n",
        "        # print(i, ' ----> ', loss)\n",
        "        losses.append(loss)\n",
        "        # Backpropagation\n",
        "        dL_dZ2 = A[-1] - Y\n",
        "        dL_dZ = [dL_dZ2]\n",
        "        # here the variable indice has for aim to keep truck to which layer are we\n",
        "        # and the variable indx_act_func has the objectif to tell us which activation function should we use in each layer\n",
        "        indice, indx_act_func = 0, -1\n",
        "        # my_reverse_cnn = my_cnn[::-1] # reverse my_cnn and get a copy\n",
        "        dL_dW, dL_dB = backpropagation(my_cnn, dL_dZ, W, Z, A )\n",
        "        # if i >= 15:\n",
        "        #     print(Z[1])\n",
        "          \n",
        "        # update weights W and Biais B  \n",
        "        dL_dW.reverse()\n",
        "        dL_dB.reverse()\n",
        "        lr = my_cnn[-1]['learning_rate']\n",
        "        \n",
        "        W, B = update_W_and_B(W, dL_dW, B, dL_dB, lr)\n",
        "        \n",
        "\n",
        "\n",
        "        # dL_dW, dL_dB = backpropagation(Z, A, W)\n",
        "        # Update W and B\n",
        "        # W, B = update_weights_and_biais(W, dL_dW, B, dL_dB, 0.01)\n",
        "        # W, B = initialization(my_cnn)\n",
        "    print('accuracy = ',compute_accuracy(my_cnn, val_images, val_labels, W, B))\n",
        "    all_losses.append(mean(losses))\n",
        "    print(\"epoch num : \",epoch,\" loss : \",mean(losses), \" --------------------> time_epoch : \", time.time() - start_time)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy =  0.663\n",
            "epoch num :  0  loss :  6.31632440795592  --------------------> time_epoch :  1531.0097241401672\n",
            "accuracy =  0.6576\n",
            "epoch num :  1  loss :  7.045506429758573  --------------------> time_epoch :  1533.523036956787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4e186ca6ffd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mindice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindx_act_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# my_reverse_cnn = my_cnn[::-1] # reverse my_cnn and get a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdL_dW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdL_dB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdL_dZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# if i >= 15:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#     print(Z[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-788d42a9df63>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(my_cnn, dL_dZ, W, Z, A)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iterate through all layers from output to input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdl_dw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_dz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdL_dZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(num_layer, operation_types[num_layer], dl_dw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdL_dW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_dw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6bd0e933ea8e>\u001b[0m in \u001b[0;36mbackpro_pooling\u001b[0;34m(my_cnn, dL_dZ, W, Z, A, num_layer)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m# imaget_max_value = np.max(imaget)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mligne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolonne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimaget_max_index\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0msize_of_pooling_kernel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mimaget_max_index\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msize_of_pooling_kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mimaget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimaget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mimaget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mligne\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolonne\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mdl_dz_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize_of_pooling_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize_of_pooling_kernel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimaget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nsxe97eJOvh"
      },
      "source": [
        "# /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
        "# accuracy =  0.3798\n",
        "# epoch num :  0  loss :  3.75313478782848  --------------------> time_epoch :  1643.9511756896973\n",
        "# accuracy =  0.4826\n",
        "# epoch num :  1  loss :  1.882252348539035  --------------------> time_epoch :  1642.2387099266052\n",
        "# accuracy =  0.5164\n",
        "# epoch num :  2  loss :  1.6416689079660005  --------------------> time_epoch :  1629.0761847496033\n",
        "# accuracy =  0.5446\n",
        "# epoch num :  3  loss :  1.532301295003278  --------------------> time_epoch :  1626.1882090568542"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "_af1gXfishZz",
        "outputId": "67454001-b2d5-4dbd-bce1-28868ee36398"
      },
      "source": [
        "losses"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-df3c821d0997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6EM7zijshT5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h-_wbZxshLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}