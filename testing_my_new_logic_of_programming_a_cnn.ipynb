{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing_my_new_logic_of_programming_a_cnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrkCByww5Zo1N+fJenVqbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syphaxAouadene/Cours_programmation_concurrente/blob/main/testing_my_new_logic_of_programming_a_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_NqPvaKWWhK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2do1QelMSg1P",
        "outputId": "d052320a-cb21-4d62-d2c2-b350d86961b9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "import os\n",
        "from scipy import signal\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "import platform"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg09QpqgtTza",
        "outputId": "61657621-08cf-4d1a-b18e-f393a0719146"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzTv75OY8GEq"
      },
      "source": [
        "def get_convolved_layer(prev_layer, filters, biais=0, pad=0, stride=1, mode='valid'):\n",
        "    '''\n",
        "    prev_layer : is 4 dimension np array with shape(batch_size, nbr_channels, height, width)\n",
        "    filters : is a 4 dimension np array with shape(nbr_of_filters, filter_depth, filter_size, filter_size)\n",
        "    biais : is a 2 dimension np array with shape(nbr_of_filters, 1)\n",
        "    '''\n",
        "    \n",
        "    # print('nbr_filters = ',nbr_filters)\n",
        "    # calculate dimensions of output\n",
        "    if mode == 'backpro':\n",
        "        X = prev_layer\n",
        "        dl_dz = filters\n",
        "        filter_size =  X.shape[2]-dl_dz.shape[2]+1\n",
        "        dL_dF = np.zeros((dl_dz.shape[1], X.shape[1], filter_size, filter_size))\n",
        "        nbr_filters = dL_dF.shape[0]\n",
        "        batch_size = X.shape[0]\n",
        "        # print('dL_dF.shape = ',dL_dF.shape)\n",
        "        for n in range(batch_size):\n",
        "            for dz, i in zip(dl_dz[n], range(nbr_filters)):\n",
        "                dz = dz.reshape((1,)+dz.shape)\n",
        "                # print('dz shape = ',dz.shape)\n",
        "                # print('X_n.shape = ',X[n].shape)\n",
        "                # print('dL_dF_i.shape = ',dL_dF[i].shape)\n",
        "                dL_dF[i] = signal.convolve(X[n], dz, mode='valid')\n",
        "        convolved_layer = dL_dF\n",
        "    \n",
        "    elif mode == 'valid':\n",
        "        # retreive dimensions of previous_layer(i.e. prev_layer)\n",
        "        batch_size, depth_prev_map, height_prev_map, width_prev_map = prev_layer.shape\n",
        "        # retreive dimensions of filters(i.e. filters)\n",
        "        nbr_filters, depth_filter, height_filter, width_filter= filters.shape\n",
        "        height = int(((height_prev_map - height_filter + 2*pad)/stride) + 1)\n",
        "        width = int(((width_prev_map - width_filter + 2*pad)/stride) + 1)\n",
        "        convolved_layer = np.zeros((batch_size, nbr_filters, height, width))\n",
        "        for n in range(batch_size):\n",
        "            for f, i in zip(filters, range(nbr_filters)):\n",
        "                convolved_layer[n][i] = signal.convolve(prev_layer[n], f, mode=mode) + biais[i]\n",
        "    elif mode == 'full':\n",
        "        # print('chui entré')\n",
        "        dl_dz = prev_layer\n",
        "        # retreive dimensions of previous_layer(i.e. prev_layer)\n",
        "        batch_size, depth_prev_map, height_prev_map, width_prev_map = prev_layer.shape\n",
        "        # retreive dimensions of filters(i.e. filters)\n",
        "        nbr_filters, depth_filter, height_filter, width_filter= filters.shape\n",
        "        \n",
        "        height = int(((height_prev_map + height_filter + 2*pad)/stride) - 1)\n",
        "        width = int(((width_prev_map + width_filter + 2*pad)/stride) - 1)\n",
        "        full_convolved = np.zeros((batch_size, depth_filter, height, width))\n",
        "        \n",
        "        \n",
        "        for n in range(batch_size):\n",
        "            for f, i in zip(filters, range(nbr_filters)):\n",
        "                # print(i)\n",
        "                one_eror_map = dl_dz[n][i].reshape((1,)+ dl_dz[n][i].shape)\n",
        "                full_convolved[n] += signal.convolve(one_eror_map, np.rot90(f, 2), mode='full')\n",
        "        \n",
        "        # print('full_convolved = ',full_convolved.shape)\n",
        "        convolved_layer = full_convolved\n",
        "        \n",
        "        \n",
        "        # print('full')\n",
        "        # height = int(((height_prev_map + height_filter + 2*pad)/stride) - 1)\n",
        "        # width = int(((width_prev_map + width_filter + 2*pad)/stride) - 1)\n",
        "        # convolved_layer = np.zeros((batch_size, nbr_filters, height, width))\n",
        "        # print(convolved_layer.shape)\n",
        "        # for n in range(batch_size):\n",
        "        #     for f, i in zip(filters, range(nbr_filters)):\n",
        "        #         convolved_layer[n][i] = signal.convolve(prev_layer[n], f, mode=mode)\n",
        "    else:\n",
        "        print('Erreur : mode not understood ! --> mode has to be \"valid\" or \"full\"')\n",
        "    return convolved_layer\n",
        "\n",
        "\n",
        "def one_hot(y):\n",
        "    y = int(y)\n",
        "    return np.eye(10)[y].reshape(10, 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlWU_fFbeY8i"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqhlMzu3C7Y"
      },
      "source": [
        "# Upload_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOHUx7_i42LZ"
      },
      "source": [
        "def normelize(img):\n",
        "    return (img/255) - 0.5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h0fTXeKDcrd"
      },
      "source": [
        "images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-images.idx3-ubyte'\n",
        "labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-labels.idx1-ubyte'\n",
        "test_images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-images.idx3-ubyte'\n",
        "test_labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-labels.idx1-ubyte'\n",
        "test_images, test_labels = loadlocal_mnist(test_images_path, test_labels_path)\n",
        "train_images, train_labels = loadlocal_mnist(images_path, labels_path)\n",
        "\n",
        "# group all the images in one list\n",
        "# then normelize all the images\n",
        "images = np.concatenate([train_images, test_images])\n",
        "labels = np.concatenate([train_labels, test_labels])\n",
        "images = normelize(images)\n",
        "\n",
        "# shuffle all the images and all labels randomly\n",
        "random.seed(1331)\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "labels = labels[indices]\n",
        "images = images[indices]\n",
        "\n",
        "# change shape of the images\n",
        "images = images.reshape(len(images), 1, 28, 28)\n",
        "\n",
        "# split the data into train, validation and test \n",
        "train_images, val_images, test_images = images[:60000], images[60000:65000], images[65000:]\n",
        "train_labels, val_labels, test_labels = labels[:60000], labels[60000:65000], labels[65000:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKDs1n09NiiD"
      },
      "source": [
        "def input_layer(dict):\n",
        "    dict['type_of_layer'] = 'input'\n",
        "    return dict\n",
        "\n",
        "def convolution_layer(dict):\n",
        "    dict['type_of_layer'] = 'convolution'\n",
        "    return dict\n",
        "\n",
        "def pooling_layer(dict):\n",
        "    dict['type_of_layer'] = 'pooling'\n",
        "    return dict\n",
        "\n",
        "def flatten_layer():\n",
        "    dict = {'type_of_layer': 'flatten'}\n",
        "    return dict\n",
        "\n",
        "def fcl(dict):\n",
        "    dict['type_of_layer'] = 'fcl'\n",
        "    return dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9U6VqX3fxB1"
      },
      "source": [
        "def initialize_filters(nbr_of_filters, filter_depth, filter_size):\n",
        "    \"\"\"\n",
        "    cette fonction s'occupe de l'initialisation d'un filtre aléatoirement selon la distribution normale\n",
        "    \"\"\"\n",
        "    # ca serait bien d'ajouter d'autres choix d'initialisations\n",
        "    return np.random.randn(nbr_of_filters, filter_depth, filter_size, filter_size)\n",
        "\n",
        "def initialize_weights(nbr_of_neurons, nbr_neurons_prev_layer):\n",
        "    \"\"\"\n",
        "    this function initialize a matrix of weights and return it\n",
        "    \"\"\"\n",
        "    # ca serait bien d'ajouter d'autres choix d'initialisations\n",
        "    return np.random.randn(nbr_of_neurons, nbr_neurons_prev_layer)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax2R5dFpd8Rb"
      },
      "source": [
        "def input_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def convolution_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    nbr_of_filters = my_cnn[num_layer]['nbr_of_kernels']\n",
        "    filter_depth = my_cnn[num_layer - 1]['depth']\n",
        "    filter_size =  my_cnn[num_layer]['kernel_size']\n",
        "    height_prev_map = my_cnn[num_layer - 1]['height']\n",
        "    width_prev_map = my_cnn[num_layer - 1]['width']\n",
        "    pad = my_cnn[num_layer]['padding']\n",
        "    stride = my_cnn[num_layer]['stride']\n",
        "    \n",
        "    # add new informations to the layer such as : depth, height and width of the image at this level\n",
        "    my_network[num_layer]['depth'] = nbr_of_filters\n",
        "    my_network[num_layer]['height'] = int(((height_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "    my_network[num_layer]['width'] = int(((width_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "\n",
        "    w = initialize_filters(nbr_of_filters, filter_depth, filter_size)\n",
        "    b = initialize_weights(nbr_of_filters, 1)\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def pooling_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    height_prev_map = my_cnn[num_layer - 1]['height']\n",
        "    width_prev_map = my_cnn[num_layer - 1]['width']\n",
        "    pad = my_cnn[num_layer]['padding']\n",
        "    stride = my_cnn[num_layer]['stride']\n",
        "    filter_size = my_cnn[num_layer]['kernel_size']\n",
        "\n",
        "    # add new informations to the layer such as : depth, height and width of the image at this level\n",
        "    my_network[num_layer]['depth'] = my_cnn[num_layer - 1]['depth']\n",
        "    my_network[num_layer]['height'] = int(((height_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "    my_network[num_layer]['width'] = int(((width_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "\n",
        "    # there is no weights neither biais in this pooling layer, so we return None to each of them\n",
        "    w, b = None, None \n",
        "    return w, b\n",
        "\n",
        "\n",
        "def flatten_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    depth_prev_map = my_cnn[num_layer - 1]['depth']\n",
        "    height_prev_map = my_cnn[num_layer - 1]['height']\n",
        "    width_prev_map = my_cnn[num_layer - 1]['width']\n",
        "\n",
        "    # add new informations to the layer such as : depth (i.e. nbrs of neurons in this flatten layer at this level)\n",
        "    my_network[num_layer]['depth'] = height_prev_map * width_prev_map * depth_prev_map\n",
        "\n",
        "    # there is no weights neither biais in this pooling layer, so we return None to each of them\n",
        "    w, b = None, None \n",
        "    return w, b\n",
        "\n",
        "\n",
        "def fcl_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    nbr_neurons = my_cnn[num_layer]['nbr_of_neurons']\n",
        "    nbr_neurons_prev_layer = my_cnn[num_layer - 1]['depth']\n",
        "    # add new informations to the layer such as : depth (i.e. nbrs of neurons in this flatten layer at this level)\n",
        "    my_network[num_layer]['depth'] = nbr_neurons\n",
        "\n",
        "    # initialize neurons and biais of this particular layer\n",
        "    w = initialize_weights(nbr_neurons, nbr_neurons_prev_layer)\n",
        "    b = initialize_weights(nbr_neurons, 1)\n",
        "    return w, np.array(b)\n",
        "\n",
        "\n",
        "def initialization(my_cnn):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'input': input_init_W_and_B,\n",
        "        'convolution': convolution_init_W_and_B,\n",
        "        'pooling': pooling_init_W_and_B,\n",
        "        'flatten': flatten_init_W_and_B,\n",
        "        'fcl' : fcl_init_W_and_B\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_layer\") for type_of_layer in my_cnn_architecture]\n",
        "    W, B = [], []\n",
        "    for i in range(len(my_cnn)):\n",
        "        w, b = operation_types[i](my_cnn, i) # variable i represents num_layer\n",
        "        W.append(w)\n",
        "        B.append(b)\n",
        "    return W, B\n",
        "\n",
        "def ReLU(layer):\n",
        "    return layer * (layer > 0)\n",
        "\n",
        "\n",
        "def d_ReLU(layer):\n",
        "    return 1. * (layer > 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y58KaYUjb302"
      },
      "source": [
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11-aJwMK8muV"
      },
      "source": [
        "def max_pooling(prev_map, size_of_pooling_kernel, pad, stride):\n",
        "#     result_of_pooling has to have shape = ((input_width - kernel_width + 2*padding)/stride) + 1\n",
        "    pooled_map = np.zeros((int((prev_map.shape[0]-size_of_pooling_kernel+2*pad)/stride)+1, int((prev_map.shape[1]-size_of_pooling_kernel+2*pad)/stride)+1))\n",
        "  \n",
        "    dP_dC = np.zeros(prev_map.shape, dtype=np.float64)\n",
        "    for i in range(pooled_map.shape[0]):\n",
        "        for j in range(pooled_map.shape[1]):\n",
        "            imaget = prev_map[stride*i:size_of_pooling_kernel+stride*i, stride*j:size_of_pooling_kernel+stride*j]\n",
        "            max_indices = np.unravel_index(np.argmax(imaget), imaget.shape)\n",
        "            max_imaget = imaget[max_indices[0]][max_indices[1]]\n",
        "            pooled_map[i][j] = max_imaget\n",
        "            dP_dC[stride * i + max_indices[0]][stride * j + max_indices[1]] = 1 \n",
        "    return pooled_map, dP_dC"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GpgmDd5LKug"
      },
      "source": [
        "def flatten(img):\n",
        "    flattened_img = img.flatten()\n",
        "    length = len(flattened_img)\n",
        "    flattened_img = flattened_img.reshape(length, 1)\n",
        "    return flattened_img"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kjo69kkeb2D"
      },
      "source": [
        "def forward_propagation(img, my_cnn, W, B):\n",
        "    \"\"\"\n",
        "    - img : np.array(shape=(1, 1, 28, 28))\n",
        "    - W : is a list of numpy arrays\n",
        "    - B : is a list of numpy arrays\n",
        "    \"\"\"\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'convolution': convolution_operation,\n",
        "        'pooling': pooling_operation,\n",
        "        'flatten': flatten_operation,\n",
        "        'fcl' : fcl_operation\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_layer\") for type_of_layer in my_cnn_architecture]\n",
        "    Z, A = [img], [img]\n",
        "    dP_dC = []\n",
        "    for i in range(1, len(my_cnn)):\n",
        "        z, a, dp_dc = operation_types[i](my_cnn[i], A, W, B, i)\n",
        "        Z.append(z)\n",
        "        A.append(a)\n",
        "        if type(dp_dc) != type(None):\n",
        "            dP_dC.append(dp_dc)\n",
        "#     print(dP_dC)\n",
        "#     dP_dC = np.array(dP_dC)\n",
        "    return Z, A, dP_dC\n",
        "\n",
        "\n",
        "def convolution_operation(layer, A, W, B, num_layer): \n",
        "    # print('convolution_operation ',layer )\n",
        "    prev_layer = A[-1]\n",
        "    filters = W[num_layer]\n",
        "    biais = B[num_layer]\n",
        "    pad = layer['padding']\n",
        "    stride = layer['stride']\n",
        "    z = get_convolved_layer(prev_layer, filters, biais, pad=pad, stride=stride, mode='valid')\n",
        "    a = ReLU(z)\n",
        "    # print(a.shape)\n",
        "    return z, a, None\n",
        "\n",
        "\n",
        "def pooling_operation(layer, A, W, B, layer_num):\n",
        "    \"\"\"\n",
        "    convolved_layer : is a list that contains each convolved_map from previous_layer\n",
        "    type_of_pooling : should be either 'MAX_POOLING' or 'MEAN_POOLING' or 'MIN_POOLING'\n",
        "    size_of_pooling_kernel : is an integer that represents the shape of kernel \n",
        "                            (if size_of_pooling_kernel=2 then shape_kernel=(2, 2))\n",
        "    this function return a list that contains each pooled_map\n",
        "    \"\"\"\n",
        "    prev_layer = A[-1]\n",
        "    batch_size = prev_layer.shape[0]\n",
        "    type_of_pooling = layer['type_of_pooling']\n",
        "    size_of_pooling_kernel = layer['kernel_size']\n",
        "    stride = layer['stride']\n",
        "    pad = layer['padding']\n",
        "    height = layer['height']\n",
        "    width = layer['width']\n",
        "    depth = layer['depth']\n",
        "    pooled_layer = np.zeros((batch_size, depth, height, width))\n",
        "    switcher = {\n",
        "        'MAX_POOLING': max_pooling,\n",
        "        'MEAN_POOLING': mean_pooling,\n",
        "        'MIN_POOLING': min_pooling\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    pooling_function = switcher.get(type_of_pooling, lambda: \"Invalid type_of_pooling !\")\n",
        "    # Execute the function\n",
        "    height_prev_map, width_prev_map = prev_layer.shape[2], prev_layer.shape[3]\n",
        "    dP_dC = np.zeros((batch_size, depth, height_prev_map, width_prev_map))\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        for d in range(depth):\n",
        "            pooled_layer[i, d], dP_dC[i, d] = pooling_function(prev_layer[i, d], size_of_pooling_kernel, pad, stride)\n",
        "            \n",
        "    return pooled_layer, pooled_layer, dP_dC\n",
        "\n",
        "\n",
        "def flatten_operation(layer, A, W, B, layer_num):\n",
        "    a = flatten(A[-1])   \n",
        "    return a, a, None\n",
        "\n",
        "\n",
        "def fcl_operation(layer, A, W, B, layer_num):\n",
        "    # global W\n",
        "    # global compteur\n",
        "    # compteur += 1\n",
        "    input_fcl = A[-1]\n",
        "    weights_fcl = W[layer_num]\n",
        "    biais_fcl = B[layer_num]\n",
        "    type_of_activation = layer['type_of_activation'].lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid,\n",
        "        'softmax': softmax\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' or 'softmax' !\")\n",
        "    \n",
        "    output_fcl = np.dot(weights_fcl, input_fcl) + biais_fcl\n",
        "    # print('shape output_fcl = ',output_fcl.shape)\n",
        "    output = activation_type(output_fcl)\n",
        "    # print(W)\n",
        "    return output_fcl, output, None"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgg8fIxSR0RK"
      },
      "source": [
        "def tanh(layer):\n",
        "    r = (np.exp(layer)-np.exp(-1*layer))/(np.exp(layer)+np.exp(-1*layer))   \n",
        "    return np.array(r)\n",
        "\n",
        "\n",
        "def d_tanh(layer):\n",
        "    return 1 - tanh(layer) * tanh(layer)\n",
        "\n",
        "\n",
        "def segmoid(layer):\n",
        "    return np.exp(layer)/(1 + np.exp(layer))\n",
        "\n",
        "\n",
        "def d_segmoid(vector):\n",
        "    \"\"\"\n",
        "    cette fontion prend un vector en entrée et retourne la dérivée de segmoid par rapport a ce vector\n",
        "    \"\"\"\n",
        "    return segmoid(vector) * (1 - segmoid(vector))\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    maxi = np.max(x)\n",
        "    return np.exp(x-maxi)/np.sum(np.exp(x-maxi))\n",
        "\n",
        "\n",
        "def categoricalCrossEntropy(generated_values, target_values):\n",
        "    generated_values = [[1.0e-100] if r[0]==0.0 else r for r in generated_values]\n",
        "    somme = 0\n",
        "    for i in range(len(generated_values)):\n",
        "        somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "    return (-1) * somme \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bk9bI3QSnTk"
      },
      "source": [
        "# def sy_fit(train_images, train_labels, my_cnn, hyper_params):\n",
        "#     \"\"\"\n",
        "#     This function train the model with train_data and train_labels\n",
        "#     and return the trained model as a list of [Weights, Biais, Losses, accuracies, ...]\n",
        "\n",
        "#     - train_data : np.array(n_images, depth, height, width)\n",
        "#     - train_labels : array of int/str ----> np.array([label1, label2, ...])\n",
        "#     - my_cnn : a list of dictionnaries, that describe your architecture\n",
        "#     - hyper_params : a dictionnary {'nbr_of_epochs': 20 ---> int\n",
        "#         'learning_rate': 0.005, ---> float\n",
        "#         'batch_learning': False, ---> bool\n",
        "#         'batch_size': 10, ---> int\n",
        "#         'drop-out': False, ---> bool\n",
        "#         'drop-out_value': 0.1 ---> float(probability)} \n",
        "#     \"\"\"\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0jzCFNZYyqd"
      },
      "source": [
        "def full_convolution(img, f):\n",
        "    return signal.convolve(img, f, mode='full')\n",
        "\n",
        "\n",
        "def unflatten(vector, pooled_layer):\n",
        "    vector = vector.reshape(pooled_layer.shape)\n",
        "    return vector\n",
        "\n",
        "\n",
        "def mean_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.mean(imaget)\n",
        "    return result\n",
        "    \n",
        "    \n",
        "def min_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.min(imaget)\n",
        "    return result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBDSescOrfk3"
      },
      "source": [
        "def update_W_and_B(W, dL_dW, B, dL_dB, lr):\n",
        "    \"\"\"\n",
        "    this function update the weights and Biais of myNetwork\n",
        "    arguments : \n",
        "    - W : it is a list that contains each Weight vector ([W1, W2, ...])\n",
        "    - dL_dW : derivatives of loss with respect to Weights (it is a list that contains Weights derivatives vectors [dL_dW1, dL_dW2, ...])\n",
        "    - B : it is a list that contains each Biais vector ([B1, B2, ...])\n",
        "    - dL_dB : derivatives of loss with respect to Biais (it is a list that contains Biais derivatives vectors [dL_dB1, dL_dB2, ...])\n",
        "    - lr : learning rate (real number)\n",
        "    \"\"\"\n",
        "    new_W = []\n",
        "    new_B = []\n",
        "#     print(' W = {} \\n\\n\\n dW = {}'.format((W), dL_dW))\n",
        "    for w, dw in zip(W, dL_dW):\n",
        "        try:\n",
        "            w = w - lr * dw\n",
        "        except:\n",
        "#             print(w.shape,dw.shape)\n",
        "#             print('except')\n",
        "            w = None\n",
        "        \n",
        "        new_W.append(w)\n",
        "    for b, db in zip(B, dL_dB):\n",
        "        try:\n",
        "            b = b - lr * db\n",
        "        except:\n",
        "            b = None\n",
        "        \n",
        "        new_B.append(b)\n",
        "        \n",
        "    return new_W, new_B"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrN_Q4FZPW7"
      },
      "source": [
        "def backpro_input(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "#     print('backpro_input')\n",
        "    return None, None, None\n",
        "\n",
        "def backpro_convolution(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC): # le probléme peut se poser ici\n",
        "    F = W[num_layer]\n",
        "    dl_dz = d_ReLU(Z[num_layer]) * dL_dZ[-1]\n",
        "    dL_dF = np.zeros(W[num_layer].shape)\n",
        "    dL_dX = np.zeros(A[num_layer-1].shape)\n",
        "    filter_size = my_cnn[num_layer].get('kernel_size')\n",
        "    batch_size = A[num_layer-1].shape[0]\n",
        "    dL_dB = np.zeros((my_cnn[num_layer]['depth'], 1))\n",
        "  \n",
        "    for n in range(dl_dz.shape[0]):\n",
        "        for d in range(dl_dz.shape[1]):\n",
        "            dL_dB[d] = np.sum(dl_dz[n][d])\n",
        "    X = A[num_layer - 1]\n",
        "    \n",
        "    dL_dX = get_convolved_layer(dl_dz, F, mode='full')\n",
        "\n",
        "    dL_dF = get_convolved_layer(X, dl_dz, mode='backpro')\n",
        "    return dL_dF, dL_dB, dL_dX\n",
        "    \n",
        "\n",
        "\n",
        "def backpro_pooling(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "#     print('backpro_pooling')\n",
        "    if my_cnn[num_layer - 1].get('type_of_layer') == 'pooling':\n",
        "        print('here i have to calculate max indices matrix')\n",
        "    else:\n",
        "#         print('everything is okay !')\n",
        "        size_of_pooling_kernel = my_cnn[num_layer]['kernel_size']\n",
        "        stride = my_cnn[num_layer]['stride']\n",
        "        dl_dz = dL_dZ[-1] \n",
        "        dp_dc = dP_dC \n",
        "        result = np.zeros(dp_dc.shape)\n",
        "#         print('lenght dp_dc = ', len(dp_dc))\n",
        "        \n",
        "        for n in range(dp_dc.shape[0]):\n",
        "            for d in range(dp_dc.shape[1]):\n",
        "                i = 0\n",
        "                while i in range(dl_dz.shape[2]):\n",
        "                    j = 0\n",
        "                    while j in range(dl_dz.shape[3]):\n",
        "                        imaget = dp_dc[n][d][i*stride:i*stride+size_of_pooling_kernel, \\\n",
        "                                            j*stride:j*stride+size_of_pooling_kernel]\n",
        "                        rows, cols = np.where(imaget == 1)\n",
        "                        i_max, j_max = rows[0], cols[0]\n",
        "                        result[n][d][stride*i+i_max][stride*j+j_max] = dl_dz[n][d][i][j]\n",
        "                        j = j + 1\n",
        "                    i = i + 1\n",
        "      \n",
        "    return None, None, result\n",
        "\n",
        "\n",
        "def backpro_flatten(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "#     print('backpro_flatten')\n",
        "    dl_dz = unflatten(dL_dZ[-1], A[num_layer-1])\n",
        "    return None, None, dl_dz\n",
        "\n",
        "\n",
        "def backpro_fcl(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "#     print('backpro_fcl')\n",
        "    type_of_activation = my_cnn[num_layer-1].get('type_of_activation')\n",
        "    \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    if type_of_activation != None:\n",
        "        activation_type = switcher.get(type_of_activation, lambda: None)\n",
        "\n",
        "    \n",
        "    dl_dw = np.dot(dL_dZ[-1], np.transpose(A[num_layer - 1]))\n",
        "    dl_db = dL_dZ[-1]\n",
        "    dl_da = np.dot(np.transpose(W[num_layer]), dL_dZ[-1])\n",
        "    \n",
        "    if type_of_activation == None:\n",
        "        dl_dz = dl_da\n",
        "    else:\n",
        "        da_dz = activation_type(Z[num_layer - 1])\n",
        "        dl_dz = dl_da * da_dz\n",
        "    return dl_dw, dl_db, dl_dz\n",
        "\n",
        "\n",
        "\n",
        "def backpropagation(my_cnn, dL_dZ, W, Z, A, dP_dC):\n",
        "    \n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "\n",
        "    switcher = {\n",
        "        'convolution': backpro_convolution,\n",
        "        'pooling': backpro_pooling,\n",
        "        'flatten': backpro_flatten,\n",
        "        'fcl' : backpro_fcl,\n",
        "        'input' : backpro_input\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_layer\") for type_of_layer in my_cnn_architecture]\n",
        "    \n",
        "    dL_dW, dL_dB = [], []\n",
        "    \n",
        "    for num_layer in range(len(my_cnn)-1, -1, -1): # iterate through all layers from output to input\n",
        "        if my_cnn[num_layer].get('type_of_layer') == 'pooling':\n",
        "            dl_dw, dl_db, dl_dz = operation_types[num_layer](my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC[-1])\n",
        "            dP_dC = dP_dC[:-1]\n",
        "        else:\n",
        "            dl_dw, dl_db, dl_dz = operation_types[num_layer](my_cnn, dL_dZ, W, Z, A, num_layer, _)\n",
        "        \n",
        "        dL_dW.append(dl_dw)\n",
        "        dL_dB.append(dl_db)\n",
        "        dL_dZ.append(dl_dz)\n",
        "    \n",
        "    return dL_dW, dL_dB"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5L-eGZxX24U"
      },
      "source": [
        "def compute_accuracy(my_cnn, x_val, y_val, W, B):\n",
        "    '''\n",
        "        This function does a forward pass of x_validation, then checks if the indices\n",
        "        of the maximum value in the output equals the indices in the label\n",
        "        y. Then it sums over each prediction and calculates the accuracy.\n",
        "    '''\n",
        "    predictions = []\n",
        "\n",
        "    for x, y in zip(x_val, y_val):\n",
        "        X = x.reshape((1,) + x.shape)\n",
        "        Y = one_hot(y)\n",
        "        \n",
        "        Z, A, dP_dC = forward_propagation(X, my_cnn, W, B)\n",
        "        output = A[-1]\n",
        "        pred = np.argmax(output)\n",
        "        predictions.append(pred == np.argmax(Y))\n",
        "\n",
        "    return np.mean(predictions)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZyL8UX-Dcy0"
      },
      "source": [
        "# here you can define your own cnn architecture :\n",
        "# you can choose any number of layers you want\n",
        "\n",
        "my_network = [input_layer({\n",
        "              'width': 28,\n",
        "              'height': 28,\n",
        "              'depth': 1   # 1 --> means gray scale, and 3 --> means rgb\n",
        "              }\n",
        "          ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':6, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "#           convolution_layer({\n",
        "#               'nbr_of_kernels':6, \n",
        "#               'kernel_size':5, \n",
        "#               'padding':0, \n",
        "#               'stride':1, \n",
        "#               'type_of_activation':'relu'\n",
        "#               }\n",
        "#           ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':12, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "          flatten_layer(),\n",
        "#           fcl({\n",
        "#               'nbr_of_neurons' : 100, # 20 neurons in hidden layer\n",
        "#               'type_of_activation' : 'tanh', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "#               'learning_rate' : 0.001\n",
        "#                 }\n",
        "#           ),\n",
        "          fcl({\n",
        "              'nbr_of_neurons' : 128, # nbr of neurons in output_layer layer\n",
        "              'type_of_activation' : 'tanh', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "          }\n",
        "          ),\n",
        "          fcl({\n",
        "              'nbr_of_neurons' : 10, # nbr of neurons in output_layer layer\n",
        "              'type_of_activation' : 'softmax', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "          }\n",
        "          )\n",
        "          ]\n",
        "# define the hyper-parameters of your model\n",
        "hyper_params = {\n",
        "    'nbr_of_epochs': 20,\n",
        "    'learning_rate': 0.005,\n",
        "    'batch_learning': False,\n",
        "    'batch_size': 10,\n",
        "    'drop-out': False,\n",
        "    'drop-out_value': 0.1 \n",
        "    }\n",
        "\n",
        "\n",
        "# W, B = sy_fit(train_images[:1000], train_labels[:1000], my_network, hyper_params)\n",
        "# y'aura d'autres fonctions : \n",
        "# sy_classify(...), sy_show_accuracies(...) "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEQ7AqcdMAPF",
        "outputId": "a3df3458-a0ff-404f-d093-5d1b22bb8e2a"
      },
      "source": [
        "# first of all, let's initialize our weights/filters and Biais of our network\n",
        "my_cnn = my_network\n",
        "W, B = initialization(my_cnn)\n",
        "nbr_of_epochs = hyper_params['nbr_of_epochs']\n",
        "learning_rate = hyper_params['learning_rate']\n",
        "all_losses = []\n",
        "for e in range(nbr_of_epochs):\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    for i in range(len(train_images)):\n",
        "    # for i in range(10):\n",
        "#         print('epoch = {}  et image {}'.format(epoch, i))\n",
        "        # Z, A = [train_images[i]], [train_images[i]]\n",
        "        X = train_images[i].reshape((1,)+train_images[i].shape)\n",
        "        Y = one_hot(train_labels[i])\n",
        "        \n",
        "        Z, A, dP_dC = forward_propagation(X, my_network, W, B)\n",
        "        # print('shape of A[-1] = ',A[-1].shape)\n",
        "        loss = categoricalCrossEntropy(A[-1], Y)\n",
        "        # print('loss = ',loss)\n",
        "        losses.append(loss)\n",
        "        # Backpropagation\n",
        "        dL_dZ2 = A[-1] - Y\n",
        "        # print(dL_dZ2.shape)\n",
        "        dL_dZ = [dL_dZ2]\n",
        "        # here the variable indice has for aim to keep truck to which layer are we\n",
        "        # and the variable indx_act_func has the objectif to tell us which activation function should we use in each layer\n",
        "        # indice, indx_act_func = 0, -1\n",
        "        dL_dW, dL_dB = backpropagation(my_network, dL_dZ, W, Z, A , dP_dC)\n",
        "        # update weights W and Biais B  \n",
        "        dL_dW.reverse()\n",
        "        dL_dB.reverse()\n",
        "        # lr = my_cnn[-1]['learning_rate']\n",
        "        # print(dL_dW[1])\n",
        "        W, B = update_W_and_B(W, dL_dW, B, dL_dB, learning_rate)\n",
        "    acc = compute_accuracy(my_cnn, val_images, val_labels, W, B)\n",
        "    all_losses.append(mean(losses))\n",
        "    print(\"epoch num : \",e,\" loss : \",mean(losses), \" ----> time_epoch : \", time.time() - start_time, '---> accuracy = ',acc)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch num :  0  loss :  1.0421823028340218  ----> time_epoch :  65.72317600250244 ---> accuracy =  0.8268\n",
            "epoch num :  1  loss :  0.4989512929380678  ----> time_epoch :  66.43422365188599 ---> accuracy =  0.8574\n",
            "epoch num :  2  loss :  0.4117963143020996  ----> time_epoch :  66.48773574829102 ---> accuracy =  0.8648\n",
            "epoch num :  3  loss :  0.3656818337221135  ----> time_epoch :  65.65912127494812 ---> accuracy =  0.8746\n",
            "epoch num :  4  loss :  0.33235546444560554  ----> time_epoch :  66.08505153656006 ---> accuracy =  0.888\n",
            "epoch num :  5  loss :  0.307229994592666  ----> time_epoch :  65.49174237251282 ---> accuracy =  0.891\n",
            "epoch num :  6  loss :  0.286445136351486  ----> time_epoch :  65.51751232147217 ---> accuracy =  0.8916\n",
            "epoch num :  7  loss :  0.26931096837148594  ----> time_epoch :  65.71456694602966 ---> accuracy =  0.898\n",
            "epoch num :  8  loss :  0.2552984365453383  ----> time_epoch :  65.44221782684326 ---> accuracy =  0.9\n",
            "epoch num :  9  loss :  0.24289627524717985  ----> time_epoch :  65.46332359313965 ---> accuracy =  0.904\n",
            "epoch num :  10  loss :  0.23358759409900248  ----> time_epoch :  65.15676426887512 ---> accuracy =  0.906\n",
            "epoch num :  11  loss :  0.22410039190173248  ----> time_epoch :  65.47291445732117 ---> accuracy =  0.9062\n",
            "epoch num :  12  loss :  0.21523925852756026  ----> time_epoch :  65.47217440605164 ---> accuracy =  0.9114\n",
            "epoch num :  13  loss :  0.20716323497077277  ----> time_epoch :  65.59408473968506 ---> accuracy =  0.9134\n",
            "epoch num :  14  loss :  0.20012102071760968  ----> time_epoch :  65.76033067703247 ---> accuracy =  0.9148\n",
            "epoch num :  15  loss :  0.19407158824806497  ----> time_epoch :  65.5133273601532 ---> accuracy =  0.917\n",
            "epoch num :  16  loss :  0.18833741201719142  ----> time_epoch :  65.71387338638306 ---> accuracy =  0.918\n",
            "epoch num :  17  loss :  0.18293611844527904  ----> time_epoch :  65.68149089813232 ---> accuracy =  0.92\n",
            "epoch num :  18  loss :  0.1779921430711674  ----> time_epoch :  65.51159858703613 ---> accuracy =  0.921\n",
            "epoch num :  19  loss :  0.17344792172926998  ----> time_epoch :  65.74671959877014 ---> accuracy =  0.921\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}