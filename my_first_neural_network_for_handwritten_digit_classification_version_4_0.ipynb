{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_first_neural_network_for_handwritten_digit_classification version 4.0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNAdvETgESdUbm74Wt7je6I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syphaxAouadene/Cours_programmation_concurrente/blob/main/my_first_neural_network_for_handwritten_digit_classification_version_4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PuX7MmP3Ns9"
      },
      "source": [
        "# Here we can choose your own architecture such as :\n",
        "\n",
        "\n",
        "*   nbr of neurons per layer\n",
        "*   nbr of layers\n",
        "*   type of activation function per layer\n",
        "*   value of learning rate\n",
        "\n",
        "but you can't choose the activation function of the last layer because since it's a problem of classification so we decide to keep it always as softmax function\n",
        "\n",
        "same thing with the loss function, we always use CCE\n",
        "\n",
        "\n",
        "\n",
        "# Here we classify a dataset of handwritten digit(mnist) with a deep neural network with 3 layers : input_layer + hidden_layer + output_layer\n",
        "\n",
        "link to download the data : colab.research.google.com/drive/1PKLEsL9G7cSTSnANAf2J6McjRiTImUNk#scrollTo=91CcRAaq0dbD\n",
        "\n",
        "# Architecture :\n",
        "- my network = [input_layer = 28*28, 1st_hidden_layer = 128, output_layer = 10]\n",
        "- activation functions = [input_layer = None, 1st_hidden_layer = tanh, output_layer = softmax]**texte en gras** \n",
        "- Loss function: we use Categorical CrossEntropy (CCE)\n",
        "- Number of epochs : 50\n",
        "- Learning rate : 0.01\n",
        "- my data = [train-set = 60000, validation-set = 5000, test-set = 5000]\n",
        "\n",
        "# Accuracies :\n",
        "- train accuracy = 99.69333333333333 %\n",
        "- val accuracy = 94.44 %\n",
        "- test accuracy = 93.78 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJVQMAlSsyCc",
        "outputId": "4d98c14e-5c68-4551-ed07-1e23edf0c8d1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYSvMPYsy05"
      },
      "source": [
        "from mlxtend.data import loadlocal_mnist\n",
        "import platform"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg09QpqgtTza",
        "outputId": "a65dbceb-f9bc-45f3-b128-24eded2ab887"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lin2XPzwsy7J"
      },
      "source": [
        "def activation_function(layer, type_of_activation='relu'):\n",
        "    type_of_activation = type_of_activation.lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' !\")\n",
        "    return activation_type(layer)\n",
        "    \n",
        "\n",
        "def ReLU(layer):\n",
        "    return layer * (layer > 0)\n",
        "\n",
        "\n",
        "def d_ReLU(layer):\n",
        "    return 1. * (layer > 0)\n",
        "\n",
        "\n",
        "def tanh(layer):\n",
        "    r = (np.exp(layer)-np.exp(-1*layer))/(np.exp(layer)+np.exp(-1*layer))   \n",
        "    return np.array(r)\n",
        "\n",
        "\n",
        "def d_tanh(layer):\n",
        "    return 1 - tanh(layer) * tanh(layer)\n",
        "\n",
        "\n",
        "def segmoid(layer):\n",
        "    return np.array(1/(1+np.exp(-1*layer)))\n",
        "\n",
        "\n",
        "def d_segmoid(vector):\n",
        "    \"\"\"\n",
        "    cette fontion prend un vector en entrée et retourne la dérivée de segmoid par rapport a ce vector\n",
        "    \"\"\"\n",
        "    return segmoid(vector) * (1 - segmoid(vector))\n",
        "\n",
        "\n",
        "def softmax(data):\n",
        "    proba_values = np.exp(data)/(np.sum(np.exp(data)))   \n",
        "    return np.array(proba_values)\n",
        "\n",
        "\n",
        "def categoricalCrossEntropy(generated_values, target_values):\n",
        "    somme = 0\n",
        "    for i in range(len(generated_values)):\n",
        "        somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "    return (-1) * somme \n",
        "\n",
        "\n",
        "def normelize(img):\n",
        "    return img/255\n",
        "\n",
        "\n",
        "def flatten(img):\n",
        "    return img.flatten()\n",
        "\n",
        "\n",
        "def show_image(img):\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def init_params(my_network):\n",
        "    nbr_layers = len(my_network) - 1\n",
        "    W, B = [], []\n",
        "    for i in range(nbr_layers):\n",
        "        W.append(np.random.randn(my_network[i+1], my_network[i]))\n",
        "        B.append(np.random.randn(my_network[i+1], 1))\n",
        "    return W, B\n",
        "\n",
        "\n",
        "def forward_pass(img, W, B):\n",
        "    \"\"\"\n",
        "    here we will use this notation :\n",
        "    Z[i] = W[i].X + B[i]\n",
        "    A[i] = activation_function(Z[i])\n",
        "    Z is a list that carries all the output of each layer\n",
        "    A is a list that carries all the output of each activation function\n",
        "    \"\"\"\n",
        "    act_functions = activation_functions[1:-1] # we omit the first element and the last one because the first activation will be None, and the last one will always be 'softmax'\n",
        "    act_functions = [type_of_activation.lower() for type_of_activation in act_functions] # lawercase all the items\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_types = [switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function\") for type_of_activation in act_functions]\n",
        "    Z, A = [], [img]\n",
        "    for i in range(len(W)):\n",
        "        if i == len(W)-1: # we have to use softmax as activation layer because we're in the last layer\n",
        "            Z.append(np.dot(W[i], A[i]) + B[i])\n",
        "            A.append(softmax(Z[i]))\n",
        "        else: # we're in hidden layer\n",
        "            Z.append(np.dot(W[i], A[i]) + B[i])\n",
        "            A.append(activation_types[i](Z[i]))\n",
        "    return Z, A\n",
        "\n",
        "\n",
        "def one_hot(y):\n",
        "    return np.eye(10)[y].reshape(10, 1)\n",
        "\n",
        "\n",
        "def update_W_and_B(W, dL_dW, B, dL_dB, lr):\n",
        "    \"\"\"\n",
        "    this function update the weights and Biais of myNetwork\n",
        "    arguments : \n",
        "    - W : it is a list that contains each Weight vector ([W1, W2, ...])\n",
        "    - dL_dW : derivatives of loss with respect to Weights (it is a list that contains Weights derivatives vectors [dL_dW1, dL_dW2, ...])\n",
        "    - B : it is a list that contains each Biais vector ([B1, B2, ...])\n",
        "    - dL_dB : derivatives of loss with respect to Biais (it is a list that contains Biais derivatives vectors [dL_dB1, dL_dB2, ...])\n",
        "    - lr : learning rate (real number)\n",
        "    \"\"\"\n",
        "    new_W = []\n",
        "    new_B = []\n",
        "    for w, dw in zip(W, dL_dW):\n",
        "        w = w - lr * dw\n",
        "        new_W.append(w)\n",
        "    for b, db in zip(B, dL_dB):\n",
        "        b = b - lr * db\n",
        "        new_B.append(b)\n",
        "    return new_W, new_B\n",
        "\n",
        "\n",
        "def compute_accuracy(x_val, y_val, W, B):\n",
        "    '''\n",
        "        This function does a forward pass of x_validation, then checks if the indices\n",
        "        of the maximum value in the output equals the indices in the label\n",
        "        y. Then it sums over each prediction and calculates the accuracy.\n",
        "    '''\n",
        "    predictions = []\n",
        "\n",
        "    for x, y in zip(x_val, y_val):\n",
        "        # prepare the input image\n",
        "        X = flatten(x)\n",
        "        X = X.reshape(len(X), 1)\n",
        "        Y = one_hot(y)\n",
        "        # forward-propagation\n",
        "        Z, A = forward_pass(X, W, B)\n",
        "        output = A[-1]\n",
        "        pred = np.argmax(output)\n",
        "        predictions.append(pred == np.argmax(Y))\n",
        "\n",
        "    return np.mean(predictions)\n",
        "\n",
        "\n",
        "def classify(img, W, B):\n",
        "    \"\"\"\n",
        "    cette fonction recois une seule image en parametre\n",
        "    et elle reçois les poids W et les Biais B et la liste des fonctions d'activations\n",
        "    et elle retourne la catégorie de l'image en entier 0..9\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    X = flatten(img)\n",
        "    X = X.reshape(len(X), 1)\n",
        "    # forward-propagation\n",
        "    Z, A = forward_pass(X, W, B)\n",
        "    output = A[-1]\n",
        "    pred = np.argmax(output)\n",
        "    return pred\n",
        "\n",
        "\n",
        "def show_accuracies(train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B):\n",
        "    \"\"\"\n",
        "    this function compute accuracy for each train-set, validation-set, and test-set\n",
        "    then print them all.\n",
        "    arguments : train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B\n",
        "    \"\"\"\n",
        "    train_accuracy = compute_accuracy(train_images, train_labels, W, B)\n",
        "    val_accuracy = compute_accuracy(val_images, val_labels, W, B)\n",
        "    test_accuracy = compute_accuracy(test_images, test_labels, W, B)\n",
        "    print(\"Accuracies :\\n\\\n",
        "    - train accuracy = {} %\\n\\\n",
        "    - val accuracy = {} %\\n\\\n",
        "    - test accuracy = {} %\".format(train_accuracy*100, val_accuracy*100, test_accuracy*100))\n",
        "\n",
        "\n",
        "def backpro_pass(dL_dZ, A, Z, W, indice, indx_act_func):\n",
        "    # we omit the first element and the last one because the first activation will be None, and the last one will always be 'softmax'\n",
        "    act_functions = activation_functions[1:-1] \n",
        "    # lawercase all the items\n",
        "    act_functions = [type_of_activation.lower() for type_of_activation in act_functions] \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_types = [switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function\") for type_of_activation in act_functions]\n",
        "    \n",
        "    dl_dw = np.dot(dL_dZ, np.transpose(A[indice]))\n",
        "    dl_db = dL_dZ\n",
        "    dl_dz = 0\n",
        "    if indice*(-1) != len(Z)+1:\n",
        "      dl_da = np.dot(np.transpose(W[indice+1]), dL_dZ)\n",
        "      da_dz = activation_types[indx_act_func](Z[indice])\n",
        "      dl_dz = dl_da * da_dz\n",
        "    return dl_dw, dl_db, dl_dz"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQfMoEksy91"
      },
      "source": [
        "images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-images.idx3-ubyte'\n",
        "labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-labels.idx1-ubyte'\n",
        "test_images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-images.idx3-ubyte'\n",
        "test_labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-labels.idx1-ubyte'\n",
        "test_images, test_labels = loadlocal_mnist(test_images_path, test_labels_path)\n",
        "train_images, train_labels = loadlocal_mnist(images_path, labels_path)\n",
        "# group all the images in one list\n",
        "# then normelize all the images\n",
        "images = np.concatenate([train_images, test_images])\n",
        "labels = np.concatenate([train_labels, test_labels])\n",
        "images = normelize(images)\n",
        "# shuffle all the images and all labels randomly\n",
        "random.seed(12)\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "labels = labels[indices]\n",
        "images = images[indices]\n",
        "# change shape of the images\n",
        "images = images.reshape(len(images), 28, 28)\n",
        "# split the data into train, validation and test \n",
        "train_images, val_images, test_images = images[:60000], images[60000:65000], images[65000:]\n",
        "train_labels, val_labels, test_labels = labels[:60000], labels[60000:65000], labels[65000:]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "91CcRAaq0dbD",
        "outputId": "aa518d4c-64f6-4399-e791-c429a4b42eb0"
      },
      "source": [
        "# Initialize my network\n",
        "my_network = [28*28, 128, 10]\n",
        "activation_functions = [None, 'tanh', 'softmax']\n",
        "number_epochs = 50\n",
        "lr = 0.01\n",
        "# Initialize weights and biais of my_network\n",
        "W, B = init_params(my_network)\n",
        "\n",
        "# training\n",
        "start_time = time.time()\n",
        "losses = []\n",
        "accuracies = []\n",
        "for epoch in range(number_epochs):\n",
        "    epoch_losses = np.array([])\n",
        "    start_epoch_time = time.time()\n",
        "    for i in range(len(train_labels)):\n",
        "        # prepare the input image\n",
        "        X = flatten(train_images[i])\n",
        "        X = X.reshape(len(X), 1)\n",
        "        Y = one_hot(train_labels[i])\n",
        "        # forward-propagation\n",
        "        Z, A = forward_pass(X, W, B)\n",
        "        loss = categoricalCrossEntropy(A[-1], Y)\n",
        "        epoch_losses = np.concatenate([epoch_losses, loss])\n",
        "# example of neural network architecture that helps to understand my notation:\n",
        "        # 28*28 ---->| 128          --->| 20           --->| 10\n",
        "        # A[0] ----->| {Z[0], A[1]} --->| {Z[1], A[2]} --->| {Z[2], A[3]}\n",
        "        # ---------->| {W[0], B[0]} --->| {W[1], B[1]} --->| {W[2], B[2]}\n",
        "        # 1st_layer->| 1st_h_layer  --->| 2nd_h_layer  --->| outpyt_layer\n",
        "\n",
        "# example of functions :\n",
        "        # A[0] = input_layer = X\n",
        "        # Z[t] = W[t] @ A[t] + B[t]    ---------> '@' means produit matriciel\n",
        "        # A[t] = act_func(Z[t-1])\n",
        "        \n",
        "\n",
        "        # backpropagation\n",
        "        dL_dZ2 = A[-1] - Y\n",
        "        dL_dW, dL_dB, dL_dZ = [], [], [dL_dZ2]\n",
        "        # here the variable indice has for aim to keep truck to which layer are we\n",
        "        # and the variable indx_act_func has the objectif to tell us which activation function should we use in each layer\n",
        "        indice, indx_act_func = 0, -1\n",
        "        for layer in range(len(my_network)-1):\n",
        "          dl_dw, dl_db, dl_dz = backpro_pass(dL_dZ[-1], A, Z, W, indice - 2, indx_act_func)\n",
        "          dL_dW.append(dl_dw)\n",
        "          dL_dB.append(dl_db)\n",
        "          dL_dZ.append(dl_dz)\n",
        "          indice = indice - 1 \n",
        "          indx_act_func = indx_act_func - 1\n",
        "          \n",
        "        # update weights W and Biais B  \n",
        "        dL_dW.reverse()\n",
        "        dL_dB.reverse()\n",
        "        W, B = update_W_and_B(W, dL_dW, B, dL_dB, lr)\n",
        "      \n",
        "    # Test my model at epoch = gama    \n",
        "    if (epoch % 5 == 0):\n",
        "        accuracy = compute_accuracy(val_images, val_labels, W, B)\n",
        "        accuracies.append(accuracy)\n",
        "        print('---------------------------------------------------------------------------> Accuracy : ',accuracies[-1])\n",
        "\n",
        "    print('epoch ',epoch,' -------> loss : ',np.array(epoch_losses).mean(), ' | time : ',(time.time() - start_epoch_time))\n",
        "    losses.append(np.array(epoch_losses).mean())\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time), ' | time : ',(time.time() - start_time)) \n",
        "fig, ax = plt.subplots(2)\n",
        "fig.suptitle('Graph of accuracy and loss')\n",
        "ax[0].plot(losses)\n",
        "ax[1].plot(accuracies)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------> Accuracy :  0.8966\n",
            "epoch  0  -------> loss :  0.8390493598120258  | time :  64.74926209449768\n",
            "epoch  1  -------> loss :  0.3404889643428423  | time :  64.09603810310364\n",
            "epoch  2  -------> loss :  0.26349823412878404  | time :  63.50837206840515\n",
            "epoch  3  -------> loss :  0.22203373388668354  | time :  63.412933111190796\n",
            "epoch  4  -------> loss :  0.1946955972228019  | time :  63.458892583847046\n",
            "---------------------------------------------------------------------------> Accuracy :  0.929\n",
            "epoch  5  -------> loss :  0.1727025950760941  | time :  64.2703754901886\n",
            "epoch  6  -------> loss :  0.155249918230738  | time :  63.67026734352112\n",
            "epoch  7  -------> loss :  0.1410472131197574  | time :  63.39596486091614\n",
            "epoch  8  -------> loss :  0.1288779469672694  | time :  63.52583694458008\n",
            "epoch  9  -------> loss :  0.11821330265881691  | time :  63.56246376037598\n",
            "---------------------------------------------------------------------------> Accuracy :  0.9402\n",
            "epoch  10  -------> loss :  0.1085696010727483  | time :  64.3902895450592\n",
            "epoch  11  -------> loss :  0.09984904406310458  | time :  63.61401891708374\n",
            "epoch  12  -------> loss :  0.09191209432446859  | time :  63.40864324569702\n",
            "epoch  13  -------> loss :  0.08456450011512269  | time :  63.80045700073242\n",
            "epoch  14  -------> loss :  0.07793103758866166  | time :  63.71895170211792\n",
            "---------------------------------------------------------------------------> Accuracy :  0.9426\n",
            "epoch  15  -------> loss :  0.0719207842624626  | time :  64.11705899238586\n",
            "epoch  16  -------> loss :  0.0665382512229613  | time :  63.59957242012024\n",
            "epoch  17  -------> loss :  0.06173911918840217  | time :  63.4404399394989\n",
            "epoch  18  -------> loss :  0.05742764901793359  | time :  63.35372066497803\n",
            "epoch  19  -------> loss :  0.0534400592006467  | time :  63.68409442901611\n",
            "---------------------------------------------------------------------------> Accuracy :  0.943\n",
            "epoch  20  -------> loss :  0.049788687873556094  | time :  64.00218081474304\n",
            "epoch  21  -------> loss :  0.04643160905430735  | time :  63.604706048965454\n",
            "epoch  22  -------> loss :  0.04332826416296601  | time :  63.82453370094299\n",
            "epoch  23  -------> loss :  0.04048451666331679  | time :  63.67143201828003\n",
            "epoch  24  -------> loss :  0.03785971657547018  | time :  63.31999182701111\n",
            "---------------------------------------------------------------------------> Accuracy :  0.9458\n",
            "epoch  25  -------> loss :  0.03540260079068579  | time :  63.63195180892944\n",
            "epoch  26  -------> loss :  0.03309271828038538  | time :  63.39952254295349\n",
            "epoch  27  -------> loss :  0.030956583789865193  | time :  63.44507193565369\n",
            "epoch  28  -------> loss :  0.028944148121534386  | time :  63.32655715942383\n",
            "epoch  29  -------> loss :  0.02704660773789669  | time :  63.59115529060364\n",
            "---------------------------------------------------------------------------> Accuracy :  0.9448\n",
            "epoch  30  -------> loss :  0.02527397002970746  | time :  64.07827687263489\n",
            "epoch  31  -------> loss :  0.023664910510959397  | time :  63.4854781627655\n",
            "epoch  32  -------> loss :  0.022127958470629393  | time :  63.37947177886963\n",
            "epoch  33  -------> loss :  0.020733681624191116  | time :  63.389628648757935\n",
            "epoch  34  -------> loss :  0.0194030535215788  | time :  63.00039744377136\n",
            "---------------------------------------------------------------------------> Accuracy :  0.946\n",
            "epoch  35  -------> loss :  0.018201110967898004  | time :  63.727197885513306\n",
            "epoch  36  -------> loss :  0.017122494348428816  | time :  63.505043029785156\n",
            "epoch  37  -------> loss :  0.01612388383138104  | time :  63.12991809844971\n",
            "epoch  38  -------> loss :  0.015211280215162682  | time :  63.15040063858032\n",
            "epoch  39  -------> loss :  0.014364538826563219  | time :  63.34376096725464\n",
            "---------------------------------------------------------------------------> Accuracy :  0.9442\n",
            "epoch  40  -------> loss :  0.013557040522069038  | time :  63.60213494300842\n",
            "epoch  41  -------> loss :  0.012797783676193135  | time :  63.35032868385315\n",
            "epoch  42  -------> loss :  0.01211807670247508  | time :  63.33260488510132\n",
            "epoch  43  -------> loss :  0.011488387894613108  | time :  63.57106971740723\n",
            "epoch  44  -------> loss :  0.010931553606860288  | time :  63.39894938468933\n",
            "---------------------------------------------------------------------------> Accuracy :  0.9442\n",
            "epoch  45  -------> loss :  0.010420629056459805  | time :  64.07929110527039\n",
            "epoch  46  -------> loss :  0.009948255378984812  | time :  63.4331214427948\n",
            "epoch  47  -------> loss :  0.009505903794897163  | time :  63.51010084152222\n",
            "epoch  48  -------> loss :  0.009090376683802087  | time :  63.756407499313354\n",
            "epoch  49  -------> loss :  0.008691071278245577  | time :  63.76275396347046\n",
            "--- 3180.7177834510803 seconds ---  | time :  3180.7177929878235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa16ba8b490>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVXPT7Yt0dfh",
        "outputId": "3f21f956-ceb8-4992-d4ba-76c62942dbd7"
      },
      "source": [
        "show_accuracies(train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies :\n",
            "    - train accuracy = 99.69333333333333 %\n",
            "    - val accuracy = 94.44 %\n",
            "    - test accuracy = 93.78 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJPY9vki0djG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gnb6LyX0dpY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAxsKck_0dsy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQmjKaJk0dvn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDzm0zn40d4H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6DhuC5m0d7j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex3Y9-0z0d-f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcg5biLx0eBb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Aeyjos0eEL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}