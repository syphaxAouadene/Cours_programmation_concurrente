{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_a05.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBSc+8Psfme3wCErUYlYNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syphaxAouadene/Cours_programmation_concurrente/blob/main/mnist_classification_using_tensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TStcdpVxCtfM"
      },
      "source": [
        "#**MNIST CLASSIFICATION USING TENSORFLOW**\n",
        "# Accuracies :\n",
        "##    - train accuracy = 95.79166666666666 %\n",
        "##    - val accuracy = 95.1 %\n",
        "##    - test accuracy = 95.38 %"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2do1QelMSg1P",
        "outputId": "fe074c95-4b83-432d-8100-6e7b98ab4d31"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "import os\n",
        "from scipy import signal\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "import platform\n",
        "import tensorflow as tf"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['indices', 'flatten', 'tanh', 'e']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg09QpqgtTza",
        "outputId": "98385da2-b769-40fa-d928-d39fef3594a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLuvtjTv2Pfm"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzTv75OY8GEq"
      },
      "source": [
        "def one_hot(y):\n",
        "    y = int(y)\n",
        "    return tf.reshape(tf.eye(10)[y], (10, 1))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqhlMzu3C7Y"
      },
      "source": [
        "# Upload_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOHUx7_i42LZ"
      },
      "source": [
        "def normelize(img):\n",
        "    return (img/255) - 0.5"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h0fTXeKDcrd"
      },
      "source": [
        "images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-images.idx3-ubyte'\n",
        "labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-labels.idx1-ubyte'\n",
        "test_images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-images.idx3-ubyte'\n",
        "test_labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-labels.idx1-ubyte'\n",
        "test_images, test_labels = loadlocal_mnist(test_images_path, test_labels_path)\n",
        "train_images, train_labels = loadlocal_mnist(images_path, labels_path)\n",
        "\n",
        "# group all the images in one list\n",
        "# then normelize all the images\n",
        "images = np.concatenate([train_images, test_images])\n",
        "labels = np.concatenate([train_labels, test_labels])\n",
        "images = normelize(images)\n",
        "\n",
        "# shuffle all the images and all labels randomly\n",
        "# random.seed(1331)\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "labels = labels[indices]\n",
        "images = images[indices]\n",
        "\n",
        "# change shape of the images\n",
        "images = images.reshape(len(images), 1, 28, 28)\n",
        "\n",
        "# split the data into train, validation and test \n",
        "train_images, val_images, test_images = images[:60000], images[60000:65000], images[65000:]\n",
        "train_labels, val_labels, test_labels = labels[:60000], labels[60000:65000], labels[65000:]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1RZV68k052Y"
      },
      "source": [
        "# --------------------------------------#\n",
        "#shifting computation to GPU\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    gpu = gpus[0]\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    tf.config.set_visible_devices(gpu, 'GPU')\n",
        "\n",
        "#--------------------------------------#"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlN288u3y8eF"
      },
      "source": [
        "train_images = tf.convert_to_tensor(train_images)\n",
        "val_images = tf.convert_to_tensor(val_images)\n",
        "test_images = tf.convert_to_tensor(test_images)\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "val_labels = tf.convert_to_tensor(val_labels)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "\n",
        "train_images = tf.cast(train_images, tf.float32)\n",
        "val_images = tf.cast(val_images, tf.float32)\n",
        "test_images = tf.cast(test_images, tf.float32)\n",
        "train_labels = tf.cast(train_labels, tf.float32)\n",
        "val_labels = tf.cast(val_labels, tf.float32)\n",
        "test_labels = tf.cast(test_labels, tf.float32)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-M1aZph0sV6",
        "outputId": "d94d3e2d-7212-4a9e-b112-b33706bd4526"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKDs1n09NiiD"
      },
      "source": [
        "def input_layer(dict):\n",
        "    dict['type_of_layer'] = 'input'\n",
        "    return dict\n",
        "\n",
        "def convolution_layer(dict):\n",
        "    dict['type_of_layer'] = 'convolution'\n",
        "    return dict\n",
        "\n",
        "def pooling_layer(dict):\n",
        "    dict['type_of_layer'] = 'pooling'\n",
        "    return dict\n",
        "\n",
        "def flatten_layer():\n",
        "    dict = {'type_of_layer': 'flatten'}\n",
        "    return dict\n",
        "\n",
        "def fcl(dict):\n",
        "    dict['type_of_layer'] = 'fcl'\n",
        "    return dict"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9U6VqX3fxB1"
      },
      "source": [
        "def initialize_filters(nbr_of_filters, filter_depth, filter_size):\n",
        "    \"\"\"\n",
        "    cette fonction s'occupe de l'initialisation d'un filtre aléatoirement selon la distribution normale\n",
        "    \"\"\"\n",
        "    # ca serait bien d'ajouter d'autres choix d'initialisations\n",
        "    return tf.random.normal((nbr_of_filters, filter_depth, filter_size, filter_size))/9.0\n",
        "\n",
        "def initialize_weights(nbr_of_neurons, nbr_neurons_prev_layer):\n",
        "    \"\"\"\n",
        "    this function initialize a matrix of weights and return it\n",
        "    \"\"\"\n",
        "    # ca serait bien d'ajouter d'autres choix d'initialisations\n",
        "    return tf.random.normal((nbr_of_neurons, nbr_neurons_prev_layer))/9.0"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax2R5dFpd8Rb"
      },
      "source": [
        "def input_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def convolution_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    nbr_of_filters = my_cnn[num_layer]['nbr_of_kernels']\n",
        "    filter_depth = my_cnn[num_layer - 1]['depth']\n",
        "    filter_size =  my_cnn[num_layer]['kernel_size']\n",
        "    height_prev_map = my_cnn[num_layer - 1]['height']\n",
        "    width_prev_map = my_cnn[num_layer - 1]['width']\n",
        "    pad = my_cnn[num_layer]['padding']\n",
        "    stride = my_cnn[num_layer]['stride']\n",
        "    \n",
        "    # add new informations to the layer such as : depth, height and width of the image at this level\n",
        "    my_network[num_layer]['depth'] = nbr_of_filters\n",
        "    my_network[num_layer]['height'] = int(((height_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "    my_network[num_layer]['width'] = int(((width_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "\n",
        "    w = initialize_filters(nbr_of_filters, filter_depth, filter_size)\n",
        "    b = initialize_weights(nbr_of_filters, 1)\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def pooling_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    height_prev_map = my_cnn[num_layer - 1]['height']\n",
        "    width_prev_map = my_cnn[num_layer - 1]['width']\n",
        "    pad = my_cnn[num_layer]['padding']\n",
        "    stride = my_cnn[num_layer]['stride']\n",
        "    filter_size = my_cnn[num_layer]['kernel_size']\n",
        "\n",
        "    # add new informations to the layer such as : depth, height and width of the image at this level\n",
        "    my_network[num_layer]['depth'] = my_cnn[num_layer - 1]['depth']\n",
        "    my_network[num_layer]['height'] = int(((height_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "    my_network[num_layer]['width'] = int(((width_prev_map - filter_size + 2*pad)/stride) + 1)\n",
        "\n",
        "    # there is no weights neither biais in this pooling layer, so we return None to each of them\n",
        "    w, b = None, None \n",
        "    return w, b\n",
        "\n",
        "\n",
        "def flatten_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    depth_prev_map = my_cnn[num_layer - 1]['depth']\n",
        "    height_prev_map = my_cnn[num_layer - 1]['height']\n",
        "    width_prev_map = my_cnn[num_layer - 1]['width']\n",
        "\n",
        "    # add new informations to the layer such as : depth (i.e. nbrs of neurons in this flatten layer at this level)\n",
        "    my_network[num_layer]['depth'] = height_prev_map * width_prev_map * depth_prev_map\n",
        "\n",
        "    # there is no weights neither biais in this pooling layer, so we return None to each of them\n",
        "    w, b = None, None \n",
        "    return w, b\n",
        "\n",
        "\n",
        "def fcl_init_W_and_B(my_cnn, num_layer):\n",
        "    global my_network\n",
        "    nbr_neurons = my_cnn[num_layer]['nbr_of_neurons']\n",
        "    nbr_neurons_prev_layer = my_cnn[num_layer - 1]['depth']\n",
        "    # add new informations to the layer such as : depth (i.e. nbrs of neurons in this flatten layer at this level)\n",
        "    my_network[num_layer]['depth'] = nbr_neurons\n",
        "\n",
        "    # initialize neurons and biais of this particular layer\n",
        "    w = initialize_weights(nbr_neurons, nbr_neurons_prev_layer)\n",
        "    b = initialize_weights(nbr_neurons, 1)\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def initialization(my_cnn):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'input': input_init_W_and_B,\n",
        "        'convolution': convolution_init_W_and_B,\n",
        "        'pooling': pooling_init_W_and_B,\n",
        "        'flatten': flatten_init_W_and_B,\n",
        "        'fcl' : fcl_init_W_and_B\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_layer\") for type_of_layer in my_cnn_architecture]\n",
        "    W, B = [], []\n",
        "    for i in range(len(my_cnn)):\n",
        "        w, b = operation_types[i](my_cnn, i) # variable i represents num_layer\n",
        "        W.append(w)\n",
        "        B.append(b)\n",
        "    return W, B\n",
        "\n",
        "def ReLU(layer):\n",
        "    r = tf.nn.relu(layer)\n",
        "    return r\n",
        "\n",
        "\n",
        "def d_ReLU(layer):\n",
        "    f = tf.cast(layer>0, tf.float32)\n",
        "    return 1.0 * f"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y58KaYUjb302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2209cfae-5d09-4ec4-e858-be57fc080d05"
      },
      "source": [
        "train_images.shape\n",
        "\n",
        "    "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11-aJwMK8muV"
      },
      "source": [
        "def max_pooling(prev_layer, size_of_pooling_kernel, pad, stride):\n",
        "#     result_of_pooling has to have shape = ((input_width - kernel_width + 2*padding)/stride) + 1\n",
        "    \n",
        "    x = tf.transpose(prev_layer, [0, 2, 3, 1])\n",
        "    p, arg = tf.nn.max_pool_with_argmax(x, size_of_pooling_kernel, strides=[stride], padding='VALID', data_format='NHWC', output_dtype=tf.dtypes.int32)\n",
        "    p = tf.transpose(p, [0, 3, 1, 2])\n",
        "    arg = tf.transpose(arg, [0, 3, 1, 2])\n",
        "    return p, arg\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GpgmDd5LKug"
      },
      "source": [
        "def flatten(img):\n",
        "    flattened_img = img.flatten()\n",
        "    length = len(flattened_img)\n",
        "    flattened_img = flattened_img.reshape(length, 1)\n",
        "    return flattened_img"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kjo69kkeb2D"
      },
      "source": [
        "def forward_propagation(img, my_cnn, W, B):\n",
        "    \"\"\"\n",
        "    - img : np.array(shape=(1, 1, 28, 28))\n",
        "    - W : is a list of numpy arrays\n",
        "    - B : is a list of numpy arrays\n",
        "    \"\"\"\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'convolution': convolution_operation,\n",
        "        'pooling': pooling_operation,\n",
        "        'flatten': flatten_operation,\n",
        "        'fcl' : fcl_operation\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_layer\") for type_of_layer in my_cnn_architecture]\n",
        "    Z, A = [img], [img]\n",
        "    dP_dC = []\n",
        "    for i in range(1, len(my_cnn)):\n",
        "        z, a, dp_dc = operation_types[i](my_cnn[i], A, W, B, i)\n",
        "        Z.append(z)\n",
        "        A.append(a)\n",
        "        if type(dp_dc) != type(None):\n",
        "            dP_dC.append(dp_dc)\n",
        "    return Z, A, dP_dC\n",
        "\n",
        "\n",
        "def convolution_operation(layer, A, W, B, num_layer): \n",
        "    prev_layer = A[-1]\n",
        "    \n",
        "    filters = W[num_layer]\n",
        "    biais = B[num_layer]\n",
        "    pad = layer['padding']\n",
        "    stride = layer['stride']\n",
        "    z = get_convolved_layer(prev_layer, filters, biais, pad=pad, stride=stride, mode='valid')\n",
        "    a = ReLU(z)\n",
        "    return z, a, None\n",
        "\n",
        "\n",
        "def pooling_operation(layer, A, W, B, layer_num):\n",
        "    \"\"\"\n",
        "    convolved_layer : is a list that contains each convolved_map from previous_layer\n",
        "    type_of_pooling : should be either 'MAX_POOLING' or 'MEAN_POOLING' or 'MIN_POOLING'\n",
        "    size_of_pooling_kernel : is an integer that represents the shape of kernel \n",
        "                            (if size_of_pooling_kernel=2 then shape_kernel=(2, 2))\n",
        "    this function return a list that contains each pooled_map\n",
        "    \"\"\"\n",
        "    \n",
        "    prev_layer = A[-1]\n",
        "    batch_size = prev_layer.shape[0]\n",
        "    type_of_pooling = layer['type_of_pooling']\n",
        "    size_of_pooling_kernel = layer['kernel_size']\n",
        "    stride = layer['stride']\n",
        "    pad = layer['padding']\n",
        "    switcher = {\n",
        "        'MAX_POOLING': max_pooling,\n",
        "        'MEAN_POOLING': mean_pooling,\n",
        "        'MIN_POOLING': min_pooling\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    pooling_function = switcher.get(type_of_pooling, lambda: \"Invalid type_of_pooling !\")\n",
        "    pooled_layer, dP_dC = pooling_function(prev_layer, size_of_pooling_kernel, pad, stride)\n",
        "    return pooled_layer, pooled_layer, dP_dC\n",
        "\n",
        "\n",
        "def flatten_operation(layer, A, W, B, layer_num):\n",
        "    \n",
        "    a = tf.reshape(A[-1], [-1]) \n",
        "    a = tf.reshape(a, a.shape+[1])\n",
        "    a = tf.cast(a, tf.float32)\n",
        "    return a, a, None\n",
        "\n",
        "\n",
        "def fcl_operation(layer, A, W, B, layer_num):\n",
        "    \n",
        "    input_fcl = A[-1]\n",
        "    weights_fcl = W[layer_num]\n",
        "    biais_fcl = B[layer_num]\n",
        "    type_of_activation = layer['type_of_activation'].lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid,\n",
        "        'softmax': softmax\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' or 'softmax' !\")\n",
        "    \n",
        "    output_fcl = tf.matmul(weights_fcl, input_fcl) + biais_fcl\n",
        "    output = activation_type(output_fcl)\n",
        "    return output_fcl, output, None"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgg8fIxSR0RK"
      },
      "source": [
        "def tanh(layer):\n",
        "    r = (np.exp(layer)-np.exp(-1*layer))/(np.exp(layer)+np.exp(-1*layer))   \n",
        "    return np.array(r)\n",
        "\n",
        "\n",
        "def d_tanh(layer):\n",
        "    return 1 - tanh(layer) * tanh(layer)\n",
        "\n",
        "\n",
        "def segmoid(layer):\n",
        "    return np.exp(layer)/(1 + np.exp(layer))\n",
        "\n",
        "\n",
        "def d_segmoid(vector):\n",
        "    \"\"\"\n",
        "    cette fontion prend un vector en entrée et retourne la dérivée de segmoid par rapport a ce vector\n",
        "    \"\"\"\n",
        "    return segmoid(vector) * (1 - segmoid(vector))\n",
        "\n",
        "\n",
        "# def softmax(x):\n",
        "#     maxi = np.max(x)\n",
        "#     return np.exp(x-maxi)/np.sum(np.exp(x-maxi))\n",
        "def softmax(x):\n",
        "    return tf.nn.softmax(x, axis=0)\n",
        "\n",
        "def categoricalCrossEntropy(generated_values, target_values):\n",
        "    generated_values = [[1.0e-12] if r[0]==0.0 else r for r in generated_values]\n",
        "    somme = 0\n",
        "    for i in range(len(generated_values)):\n",
        "        somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "    return (-1) * somme \n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0jzCFNZYyqd"
      },
      "source": [
        "def full_convolution(img, f):\n",
        "    return signal.convolve(img, f, mode='full')\n",
        "\n",
        "\n",
        "def unflatten(vector, pooled_layer):\n",
        "    vector = vector.reshape(pooled_layer.shape)\n",
        "    return vector\n",
        "\n",
        "\n",
        "def mean_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.mean(imaget)\n",
        "    return result\n",
        "    \n",
        "    \n",
        "def min_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[i:size_of_pooling_kernel+i, j:size_of_pooling_kernel+j]\n",
        "            result[i, j] = np.min(imaget)\n",
        "    return result"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBDSescOrfk3"
      },
      "source": [
        "def update_W_and_B(W, dL_dW, B, dL_dB, lr, optimizer='sgd', V_w=0, V_b=0):\n",
        "    \"\"\"\n",
        "    this function update the weights and Biais of myNetwork\n",
        "    arguments : \n",
        "    - W : it is a list that contains each Weight vector ([W1, W2, ...])\n",
        "    - dL_dW : derivatives of loss with respect to Weights (it is a list that contains Weights derivatives vectors [dL_dW1, dL_dW2, ...])\n",
        "    - B : it is a list that contains each Biais vector ([B1, B2, ...])\n",
        "    - dL_dB : derivatives of loss with respect to Biais (it is a list that contains Biais derivatives vectors [dL_dB1, dL_dB2, ...])\n",
        "    - lr : learning rate (real number)\n",
        "    \"\"\"\n",
        "    params = W, dL_dW, B, dL_dB, lr\n",
        "    if optimizer == 'sgd':\n",
        "        new_W, new_B = sgd(params)\n",
        "        return new_W, new_B, _, _\n",
        "    elif optimizer == 'momentum':\n",
        "        new_W, new_B, new_V_w, new_V_b = momentum(params, V_w, V_b)\n",
        "        return new_W, new_B, new_V_w, new_V_b\n",
        "    else:\n",
        "        print('optimizer not understood !')\n",
        "        return -1\n",
        "\n",
        "def sgd(params):\n",
        "    W, dL_dW, B, dL_dB, lr = params\n",
        "    new_W, new_B = [], []\n",
        "    for w, dw in zip(W, dL_dW):\n",
        "        try: \n",
        "            w = w - lr * dw\n",
        "        except:\n",
        "            w = None\n",
        "        new_W.append(w)\n",
        "    for b, db in zip(B, dL_dB):\n",
        "        try:\n",
        "            b = b - lr * db\n",
        "        except:\n",
        "            b = None\n",
        "        new_B.append(b)\n",
        "    return new_W, new_B\n",
        "\n",
        "def momentum(params, V_w, V_b):\n",
        "    W, dL_dW, B, dL_dB, lr = params\n",
        "    new_W, new_B, new_V_w, new_V_b = [], [], [], []\n",
        "    for w, dw, v_w in zip(W, dL_dW, V_w):\n",
        "        try: \n",
        "            v_w = 0.9*v_w + lr*dw\n",
        "            w = w - v_w\n",
        "        except:\n",
        "            w = None\n",
        "            v_w = None\n",
        "        new_V_w.append(v_w)\n",
        "        new_W.append(w)\n",
        "    for b, db, v_b in zip(B, dL_dB, V_b):\n",
        "        try:\n",
        "            v_b = 0.9*v_b + lr*db\n",
        "            b = b - v_b\n",
        "        except:\n",
        "            b = None\n",
        "            v_b = None\n",
        "        new_B.append(b)\n",
        "        new_V_b.append(v_b)\n",
        "    return new_W, new_B, new_V_w, new_V_b"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQrN_Q4FZPW7"
      },
      "source": [
        "def backpro_input(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "    return None, None, None\n",
        "\n",
        "def backpro_convolution(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "\n",
        "    F = W[num_layer]\n",
        "    dl_dz = d_ReLU(Z[num_layer]) * dL_dZ[-1]\n",
        "    dL_dF = np.zeros(W[num_layer].shape)\n",
        "    dL_dX = np.zeros(A[num_layer-1].shape)\n",
        "    filter_size = my_cnn[num_layer].get('kernel_size')\n",
        "    batch_size = A[num_layer-1].shape[0]\n",
        "    dL_dB = np.zeros((my_cnn[num_layer]['depth'], 1))\n",
        "  \n",
        "    for n in range(dl_dz.shape[0]):\n",
        "        for d in range(dl_dz.shape[1]):\n",
        "            dL_dB[d] = np.sum(dl_dz[n][d])\n",
        "    X = A[num_layer - 1]\n",
        "    if num_layer>1:\n",
        "        dL_dX = get_convolved_layer(dl_dz, F, mode='full')\n",
        "    \n",
        "    dL_dF = get_convolved_layer(X, dl_dz, mode='backpro')\n",
        "    dL_dX = tf.cast(dL_dX, tf.float32)\n",
        "    \n",
        "    return dL_dF, dL_dB, dL_dX\n",
        "    \n",
        "\n",
        "\n",
        "def backpro_pooling(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "    \n",
        "    if num_layer>1:\n",
        "        if my_cnn[num_layer - 1].get('type_of_layer') == 'pooling':\n",
        "            print('here i have to calculate max indices matrix')\n",
        "        else:\n",
        "            prev_layer = A[num_layer-1]\n",
        "            dl_dz = dL_dZ[-1]\n",
        "            dp_dc = dP_dC\n",
        "            #############################\n",
        "            tensor = tf.zeros(prev_layer.shape)\n",
        "            tensor = tf.reshape(tensor, -1)\n",
        "            tensor = tf.reshape(tensor, (tensor.shape[0], 1))\n",
        "            tensor = tf.cast(tensor, tf.float32)\n",
        "            indices = tf.reshape(dp_dc, -1)\n",
        "            indices = tf.reshape(indices, (indices.shape[0], 1))\n",
        "            updates = tf.reshape(dl_dz, -1)\n",
        "            updates = tf.reshape(updates, (updates.shape[0], 1))\n",
        "            res = tf.tensor_scatter_nd_update(tensor, indices, updates)\n",
        "            res = tf.reshape(res, prev_layer.shape)\n",
        "            #############################\n",
        "    \n",
        "    return None, None, res\n",
        "\n",
        "\n",
        "def backpro_flatten(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "    if num_layer>1:\n",
        "        dl_dz = unflatten(dL_dZ[-1].numpy(), A[num_layer-1].numpy())\n",
        "    else:\n",
        "        dl_dz = 0 # if flatten is 1-th layer (just after 'input_layer') then we don't have to continue the backpro\n",
        "    dl_dz = tf.convert_to_tensor(dl_dz)\n",
        "    return None, None, dl_dz\n",
        "\n",
        "\n",
        "def backpro_fcl(my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC):\n",
        "    type_of_activation = my_cnn[num_layer-1].get('type_of_activation')\n",
        "    \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    if type_of_activation != None:\n",
        "        activation_type = switcher.get(type_of_activation, lambda: None)\n",
        "\n",
        "    \n",
        "    dl_dw = tf.matmul(dL_dZ[-1], tf.transpose(A[num_layer - 1]))\n",
        "    dl_db = dL_dZ[-1]\n",
        "    dl_da = tf.matmul(tf.transpose(W[num_layer]), dL_dZ[-1])\n",
        "    \n",
        "    if type_of_activation == None:\n",
        "        dl_dz = dl_da\n",
        "    else:\n",
        "        da_dz = activation_type(Z[num_layer - 1])\n",
        "        dl_dz = dl_da * da_dz\n",
        "    return dl_dw, dl_db, dl_dz\n",
        "\n",
        "\n",
        "\n",
        "def backpropagation(my_cnn, dL_dZ, W, Z, A, dP_dC):\n",
        "    \n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "\n",
        "    switcher = {\n",
        "        'convolution': backpro_convolution,\n",
        "        'pooling': backpro_pooling,\n",
        "        'flatten': backpro_flatten,\n",
        "        'fcl' : backpro_fcl,\n",
        "        'input' : backpro_input\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_layer\") for type_of_layer in my_cnn_architecture]\n",
        "    \n",
        "    dL_dW, dL_dB = [], []\n",
        "    \n",
        "    for num_layer in range(len(my_cnn)-1, -1, -1): # iterate through all layers from output to input\n",
        "        if my_cnn[num_layer].get('type_of_layer') == 'pooling':\n",
        "            dl_dw, dl_db, dl_dz = operation_types[num_layer](my_cnn, dL_dZ, W, Z, A, num_layer, dP_dC[-1])\n",
        "            dP_dC = dP_dC[:-1]\n",
        "        else:\n",
        "            dl_dw, dl_db, dl_dz = operation_types[num_layer](my_cnn, dL_dZ, W, Z, A, num_layer, _)\n",
        "        \n",
        "        dL_dW.append(dl_dw)\n",
        "        dL_dB.append(dl_db)\n",
        "        dL_dZ.append(dl_dz)\n",
        "    \n",
        "    return dL_dW, dL_dB"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5L-eGZxX24U"
      },
      "source": [
        "def compute_accuracy(my_cnn, x_val, y_val, W, B):\n",
        "    '''\n",
        "        This function does a forward pass of x_validation, then checks if the indices\n",
        "        of the maximum value in the output equals the indices in the label\n",
        "        y. Then it sums over each prediction and calculates the accuracy.\n",
        "    '''\n",
        "    predictions = []\n",
        "\n",
        "    for x, y in zip(x_val, y_val):\n",
        "        X = tf.reshape(x, (1,) + x.shape)\n",
        "        Y = one_hot(y)\n",
        "        \n",
        "        Z, A, dP_dC = forward_propagation(X, my_cnn, W, B)\n",
        "        output = A[-1]\n",
        "        pred = np.argmax(output)\n",
        "        predictions.append(pred == np.argmax(Y))\n",
        "\n",
        "    return np.mean(predictions)\n",
        "\n",
        "def show_accuracies(my_cnn, train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B):\n",
        "    \"\"\"\n",
        "    this function compute accuracy for each train-set, validation-set, and test-set\n",
        "    then print them all.\n",
        "    arguments : train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B\n",
        "    \"\"\"\n",
        "    train_accuracy = compute_accuracy(my_cnn, train_images, train_labels, W, B)\n",
        "    val_accuracy = compute_accuracy(my_cnn, val_images, val_labels, W, B)\n",
        "    test_accuracy = compute_accuracy(my_cnn, test_images, test_labels, W, B)\n",
        "    print(\"Accuracies :\\n\\\n",
        "    - train accuracy = {} %\\n\\\n",
        "    - val accuracy = {} %\\n\\\n",
        "    - test accuracy = {} %\".format(train_accuracy*100, val_accuracy*100, test_accuracy*100))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obklUVl3Vqkr"
      },
      "source": [
        "def tf_rot180(w):\n",
        "    \"\"\"\n",
        "    Roate by 180 degrees\n",
        "    \"\"\"\n",
        "    return tf.reverse(w, axis=[0, 1])\n",
        "\n",
        "\n",
        "def tf_pad_to_full_conv2d(x, w_size):\n",
        "    \"\"\"\n",
        "    Pad x, such that using a 'VALID' convolution in tensorflow is the same\n",
        "    as using a 'FULL' convolution. See\n",
        "    http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv2d\n",
        "    for description of 'FULL' convolution.\n",
        "    \"\"\"\n",
        "    return tf.pad(x, [[0, 0],\n",
        "                      [0, 0],\n",
        "                      [w_size - 1, w_size - 1],\n",
        "                      [w_size - 1, w_size - 1]\n",
        "                      ])\n",
        "\n",
        "\n",
        "\n",
        "def get_convolved_layer(prev_layer, filters, biais=0, pad=0, stride=1, mode='valid'):\n",
        "    '''\n",
        "    prev_layer : is 4 dimension np array with shape(batch_size, nbr_channels, height, width)\n",
        "    filters : is a 4 dimension np array with shape(nbr_of_filters, filter_depth, filter_size, filter_size)\n",
        "    biais : is a 2 dimension np array with shape(nbr_of_filters, 1)\n",
        "    '''\n",
        "    if mode == 'backpro':\n",
        "        \n",
        "        # --------------------------------------------------\n",
        "        X = prev_layer\n",
        "        X = tf.transpose(X, [1, 0, 2, 3])\n",
        "        dl_dz = tf.transpose(filters, [2, 3, 0, 1])\n",
        "        new_filters = tf.nn.conv2d(X, dl_dz, strides=[1,1], padding='VALID', data_format='NCHW')\n",
        "        new_filters = tf.transpose(new_filters, [1, 0, 2, 3])\n",
        "        convolved_layer = new_filters\n",
        "        # --------------------------------------------------\n",
        "        \n",
        "    \n",
        "    elif mode == 'valid':\n",
        "\n",
        "        f = tf.transpose(filters, [2, 3, 1, 0])\n",
        "        c = tf.nn.conv2d(prev_layer, f, [1], padding='VALID', data_format='NCHW')\n",
        "        convolved_layer = c\n",
        "    \n",
        "    elif mode == 'full':\n",
        "        dl_dz = prev_layer\n",
        "        f = tf.transpose(filters, [2, 3, 0, 1])\n",
        "        f = tf_rot180(f)\n",
        "        dl_dz = tf_pad_to_full_conv2d(dl_dz, f.shape[0])\n",
        "        dL_dX = tf.nn.conv2d(input=dl_dz,\n",
        "                                filters=f,\n",
        "                                strides=[1],\n",
        "                                padding='VALID',\n",
        "                                data_format='NCHW')\n",
        "\n",
        "        convolved_layer = dL_dX\n",
        "    else:\n",
        "        print('Erreur : mode not understood ! --> mode has to be \"valid\" or \"full\"')\n",
        "    return convolved_layer"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZyL8UX-Dcy0"
      },
      "source": [
        "# here you can define your own cnn architecture :\n",
        "# you can choose any number of layers you want\n",
        "\n",
        "my_network = [input_layer({\n",
        "              'width': 28,\n",
        "              'height': 28,\n",
        "              'depth': 1   # 1 --> means gray scale, and 3 --> means rgb\n",
        "              }\n",
        "          ),\n",
        "          convolution_layer({\n",
        "              'nbr_of_kernels':32, \n",
        "              'kernel_size':3, \n",
        "              'padding':0, \n",
        "              'stride':1, \n",
        "              'type_of_activation':'relu'\n",
        "              }\n",
        "          ), \n",
        "          pooling_layer({\n",
        "              'type_of_pooling' : 'MAX_POOLING',\n",
        "              'kernel_size' : 2,\n",
        "              'padding':0,\n",
        "              'stride' : 2\n",
        "              } \n",
        "          ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':24, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':48, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':62, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':120, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':150, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "        #   convolution_layer({\n",
        "        #       'nbr_of_kernels':200, \n",
        "        #       'kernel_size':5, \n",
        "        #       'padding':0, \n",
        "        #       'stride':1, \n",
        "        #       'type_of_activation':'relu'\n",
        "        #       }\n",
        "        #   ), \n",
        "        #   pooling_layer({\n",
        "        #       'type_of_pooling' : 'MAX_POOLING',\n",
        "        #       'kernel_size' : 2,\n",
        "        #       'padding':0,\n",
        "        #       'stride' : 2\n",
        "        #       } \n",
        "        #   ),\n",
        "          flatten_layer(),\n",
        "#           fcl({\n",
        "#               'nbr_of_neurons' : 100, # 20 neurons in hidden layer\n",
        "#               'type_of_activation' : 'tanh', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "#               'learning_rate' : 0.001\n",
        "#                 }\n",
        "        #   ),\n",
        "          fcl({\n",
        "              'nbr_of_neurons' : 100, # nbr of neurons in output_layer layer\n",
        "              'type_of_activation' : 'relu', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "          }\n",
        "          ),\n",
        "          fcl({\n",
        "              'nbr_of_neurons' : 10, # nbr of neurons in output_layer layer\n",
        "              'type_of_activation' : 'softmax', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "          }\n",
        "          )\n",
        "          ]\n",
        "# define the hyper-parameters of your model\n",
        "hyper_params = {\n",
        "    'nbr_of_epochs': 20,\n",
        "    'learning_rate': 0.0001,\n",
        "    'optimizer': 'momentum',\n",
        "    'batch_learning': False,\n",
        "    'batch_size': 10,\n",
        "    'drop-out': False,\n",
        "    'drop-out_value': 0.1 \n",
        "    }"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEQ7AqcdMAPF",
        "outputId": "0a5a8b1b-3db6-4bac-99eb-a898d0545a30"
      },
      "source": [
        "# first of all, let's initialize our weights/filters and Biais of our network\n",
        "my_cnn = my_network\n",
        "W, B = initialization(my_cnn)\n",
        "V_w, V_b = [], []\n",
        "for w, b in zip(W, B):\n",
        "    try:\n",
        "        V_w.append(tf.zeros(w.shape))\n",
        "        V_b.append(tf.zeros(b.shape))\n",
        "    except:\n",
        "        V_w.append(None)\n",
        "        V_b.append(None)\n",
        "nbr_of_epochs = hyper_params['nbr_of_epochs']\n",
        "learning_rate = hyper_params['learning_rate']\n",
        "optimizer = hyper_params['optimizer']\n",
        "all_losses = []\n",
        "\n",
        "for e in range(nbr_of_epochs):\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    for i in range(len(train_images)):\n",
        "        X = tf.reshape(train_images[i], (1,)+train_images[i].shape)\n",
        "        Y = one_hot(train_labels[i])\n",
        "        # forward propagation\n",
        "        Z, A, dP_dC = forward_propagation(X, my_network, W, B)\n",
        "        loss = categoricalCrossEntropy(A[-1], Y)\n",
        "        losses.append(loss)\n",
        "        # Backpropagation\n",
        "        dL_dZ = [A[-1] - Y]\n",
        "        dL_dW, dL_dB = backpropagation(my_network, dL_dZ, W, Z, A , dP_dC)\n",
        "        # update weights W and Biais B  \n",
        "        dL_dW.reverse()\n",
        "        dL_dB.reverse()\n",
        "        \n",
        "        W, B, V_w, V_b = update_W_and_B(W, dL_dW, B, dL_dB, learning_rate, optimizer, V_w, V_b)\n",
        "    acc = compute_accuracy(my_cnn, val_images, val_labels, W, B)\n",
        "    nbr_minutes = (int(time.time()-start_time))//60\n",
        "    nbr_seconds = (int(time.time()-start_time))%60\n",
        "    print('Loss ---->',round(mean(losses), 5),' |  Validation Accuracy ----->',round(acc,4),' | Time ---->',nbr_minutes,'min',nbr_seconds,'s')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss ----> 0.44026  |  Validation Accuracy -----> 0.8954  | Time ----> 34 min 27 s\n",
            "Loss ----> 0.2371  |  Validation Accuracy -----> 0.915  | Time ----> 34 min 6 s\n",
            "Loss ----> 0.18188  |  Validation Accuracy -----> 0.9324  | Time ----> 33 min 55 s\n",
            "Loss ----> 0.15335  |  Validation Accuracy -----> 0.9308  | Time ----> 33 min 51 s\n",
            "Loss ----> 0.13409  |  Validation Accuracy -----> 0.9382  | Time ----> 33 min 58 s\n",
            "Loss ----> 0.12166  |  Validation Accuracy -----> 0.9466  | Time ----> 34 min 1 s\n",
            "Loss ----> 0.11382  |  Validation Accuracy -----> 0.933  | Time ----> 33 min 52 s\n",
            "Loss ----> 0.10488  |  Validation Accuracy -----> 0.9368  | Time ----> 33 min 50 s\n",
            "Loss ----> 0.09895  |  Validation Accuracy -----> 0.941  | Time ----> 33 min 46 s\n",
            "Loss ----> 0.09483  |  Validation Accuracy -----> 0.9496  | Time ----> 33 min 52 s\n",
            "Loss ----> 0.09322  |  Validation Accuracy -----> 0.9394  | Time ----> 33 min 48 s\n",
            "Loss ----> 0.0936  |  Validation Accuracy -----> 0.9538  | Time ----> 33 min 58 s\n",
            "Loss ----> 0.09302  |  Validation Accuracy -----> 0.9518  | Time ----> 34 min 20 s\n",
            "Loss ----> 0.09538  |  Validation Accuracy -----> 0.9532  | Time ----> 33 min 58 s\n",
            "Loss ----> 0.09433  |  Validation Accuracy -----> 0.951  | Time ----> 33 min 55 s\n",
            "Loss ----> 0.0936  |  Validation Accuracy -----> 0.929  | Time ----> 33 min 56 s\n",
            "Loss ----> 0.09324  |  Validation Accuracy -----> 0.9522  | Time ----> 33 min 52 s\n",
            "Loss ----> 0.08928  |  Validation Accuracy -----> 0.9496  | Time ----> 33 min 46 s\n",
            "Loss ----> 0.09591  |  Validation Accuracy -----> 0.9352  | Time ----> 33 min 34 s\n",
            "Loss ----> 0.09494  |  Validation Accuracy -----> 0.951  | Time ----> 33 min 36 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1MXwRgoSw2H",
        "outputId": "a13af46b-4bce-439e-baa6-7f31ffa6f782"
      },
      "source": [
        "show_accuracies(my_cnn, train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies :\n",
            "    - train accuracy = 95.79166666666666 %\n",
            "    - val accuracy = 95.1 %\n",
            "    - test accuracy = 95.38 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-2bO8rbpnS0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}