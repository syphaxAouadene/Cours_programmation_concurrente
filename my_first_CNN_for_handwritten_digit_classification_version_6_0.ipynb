{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_first_CNN_for_handwritten_digit_classification version 5.0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uXOxpXATeNlp",
        "hx6wKd0xuzaj",
        "kkMiwVr_vD2f"
      ],
      "authorship_tag": "ABX9TyMuYofVdC9p3UfsGf2eY9Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syphaxAouadene/Cours_programmation_concurrente/blob/main/my_first_CNN_for_handwritten_digit_classification_version_6_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgimDyAjarpZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJVQMAlSsyCc",
        "outputId": "d765a111-1975-457f-c2d8-be2f4790e5fd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYSvMPYsy05"
      },
      "source": [
        "from mlxtend.data import loadlocal_mnist\n",
        "import platform"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg09QpqgtTza",
        "outputId": "46cda140-e641-4634-d7c7-8b92ec10378c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zLyEq5leMs8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXOxpXATeNlp"
      },
      "source": [
        "# CNN operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChRRlpbQasLv"
      },
      "source": [
        "def multiplication(imaget, f):\n",
        "    if len(imaget.shape) == 1 :\n",
        "        imaget = imaget.reshape((imaget.shape[0], 1))\n",
        "    result = imaget * f\n",
        "    somme = 0\n",
        "    for i in range(imaget.shape[0]):\n",
        "        for j in range(imaget.shape[1]):\n",
        "            somme = somme + result[i, j]\n",
        "    return somme\n",
        "\n",
        "# from scipy import signal\n",
        "# def convolution(img, f):\n",
        "#     return signal.convolve2d(img, f, mode='valid')\n",
        "\n",
        "def convolution(img, f):\n",
        "    result = np.zeros((img.shape[0] - f.shape[0] + 1, img.shape[1] - f.shape[1] + 1))\n",
        "    for i in range(result.shape[0]):\n",
        "        for j in range(result.shape[1]):\n",
        "            imaget = img[i:f.shape[0]+i, j:f.shape[1]+j]\n",
        "            multi = multiplication(imaget, f)\n",
        "            result[i, j] = multi\n",
        "    return result\n",
        "\n",
        "\n",
        "def ReLU_convolution(convolved_map):\n",
        "    result = np.zeros(convolved_map.shape)\n",
        "    for i in range(result.shape[0]):\n",
        "        for j in range(result.shape[1]):\n",
        "            result[i, j] = np.max([convolved_map[i,j], 0])\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_convolved_layer_from_previous_layer(previous_layer, nbr_filter, size_filter):\n",
        "    \"\"\"\n",
        "    don't forget to add bias term corresponding to each filter !\n",
        "    - previous_layer = is a list that contains each feature map of the previous_layer\n",
        "    - nbr_filter = is an integer that represents how many filter do we want to use, (ex. 6)\n",
        "    - size_filter = is an integer that represents the shape of each filter (if size_filter=3 then shape_filter=(3, 3))\n",
        "    \n",
        "    this function return a list that contains each convolved map \n",
        "    (you should know that convolved_map = convolution between feature_map and filter)\n",
        "    \"\"\"\n",
        "    convolved_layer = []\n",
        "    # filters = [initialize_filter(size_filter, size_filter) for i in range(nbr_filter)]\n",
        "    # biais = initialize_filter(nbr_filter, 1)\n",
        "    for f in filters:\n",
        "        somme = 0\n",
        "        bias = 0\n",
        "        for feature_map in previous_layer:\n",
        "            somme = somme + convolution(feature_map, f)\n",
        "        somme = somme + bias\n",
        "        convolved_layer.append(ReLU_convolution(somme))\n",
        "    return convolved_layer, filters, biais\n",
        "\n",
        "\n",
        "def max_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "#     result_of_pooling has to have shape = ((input_width - kernel_width + 2*padding)/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0]):\n",
        "        for j in range(0, result.shape[1]):\n",
        "            imaget = convolved_map[stride*i:size_of_pooling_kernel+stride*i, stride*j:size_of_pooling_kernel+stride*j]\n",
        "            result[i, j] = np.max(imaget)\n",
        "    # print('------------------------')\n",
        "    # print(result.shape)\n",
        "    # print('------------------------')\n",
        "    return result\n",
        "\n",
        "\n",
        "def mean_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[stride*i:size_of_pooling_kernel+stride*i, stride*j:size_of_pooling_kernel+stride*j]\n",
        "            result[i, j] = np.mean(imaget)\n",
        "    return result\n",
        "    \n",
        "    \n",
        "def min_pooling(convolved_map, size_of_pooling_kernel, stride):\n",
        "    #     result_of_pooling has to have shape = ((input_width - kernel_width) + 2*padding/stride) + 1\n",
        "    result = np.zeros((int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1, int((convolved_map.shape[0]-size_of_pooling_kernel)/stride)+1))\n",
        "    for i in range(0, result.shape[0], stride):\n",
        "        for j in range(0, result.shape[1], stride):\n",
        "            imaget = convolved_map[stride*i:size_of_pooling_kernel+stride*i, stride*j:size_of_pooling_kernel+stride*j]\n",
        "            result[i, j] = np.min(imaget)\n",
        "    return result\n",
        "\n",
        "\n",
        "def pooling(convolved_layer, type_of_pooling, size_of_pooling_kernel, stride):\n",
        "    \"\"\"\n",
        "    convolved_layer : is a list that contains each convolved_map from previous_layer\n",
        "    type_of_pooling : should be either 'MAX_POOLING' or 'MEAN_POOLING' or 'MIN_POOLING'\n",
        "    size_of_pooling_kernel : is an integer that represents the shape of kernel \n",
        "                            (if size_of_pooling_kernel=2 then shape_kernel=(2, 2))\n",
        "    this function return a list that contains each pooled_map\n",
        "    \"\"\"\n",
        "    pooled_layer = []\n",
        "    switcher = {\n",
        "        'MAX_POOLING': max_pooling,\n",
        "        'MEAN_POOLING': mean_pooling,\n",
        "        'MIN_POOLING': min_pooling\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    pooling_operation = switcher.get(type_of_pooling, lambda: \"Invalid type_of_pooling !\")\n",
        "    # Execute the function\n",
        "    for convolved_map in convolved_layer:\n",
        "        pooled_layer.append(pooling_operation(convolved_map, size_of_pooling_kernel, stride))\n",
        "    return pooled_layer\n",
        "\n",
        "\n",
        "def initialize_filter(filter_width, filter_height):\n",
        "    \"\"\"\n",
        "    cette fonction s'occupe de l'initialisation d'un filtre aléatoirement selon la distribution normale\n",
        "    \"\"\"\n",
        "    return np.random.randn(filter_width, filter_height)\n",
        "\n",
        "\n",
        "# def show_image(img):\n",
        "#     plt.imshow(img, cmap=plt.cm.binary)\n",
        "#     plt.show()\n",
        "    \n",
        "    \n",
        "def show_multiple_images(images, nbr_of_images=5):\n",
        "    for img in images[:nbr_of_images]:\n",
        "        show_image(img)\n",
        "        time.sleep(1)\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        \n",
        "# def flatten(layer):\n",
        "#     \"\"\"\n",
        "#     #arguments:\n",
        "#     layer : is a list of feature_maps\n",
        "#     #returns : list of all numbers that contained in the feature maps in the layer\n",
        "#     \"\"\"\n",
        "#     result = []\n",
        "#     for matrix in layer:\n",
        "#         result = result + list(matrix.flatten())\n",
        "#     result = np.array(result).reshape((1, len(result)))\n",
        "#     return result\n",
        "\n",
        "# def fully_connected_layer(input_layer, nbr_neurons, activation_function='ReLU'):\n",
        "#     current_layer = []\n",
        "#     input_layer = np.array(input_layer)\n",
        "#     for i in range(nbr_neurons):\n",
        "#         bias = 0\n",
        "#         weights = np.random.randn(len(input_layer), 1)\n",
        "#         current_layer.append(np.max([multiplication(input_layer, weights) + bias, 0]))\n",
        "#     return current_layer\n",
        "\n",
        "\n",
        "# def softmax(data):\n",
        "#     output = []\n",
        "#     for value in data:\n",
        "#         proba_value = np.exp(value)/(np.sum(np.exp(data)))\n",
        "#         output.append(proba_value)\n",
        "#     return output\n",
        "        \n",
        "# def categoricalCrossEntropy(generated_values, target_values):\n",
        "#     somme = 0\n",
        "#     for i in range(len(generated_values)):\n",
        "#         somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "#     return (-1) * somme  \n",
        "\n",
        "#######################################################################################\n",
        "#######  Fully_connected_layer_functions\n",
        "#######################################################################################\n",
        "\n",
        "# def update_weights(dL_dY, weights, inputs, lrate):\n",
        "#     \"\"\"\n",
        "#     arguments :\n",
        "#     dL_dY : un vecteur des dérivées de la couche supérieure par rapport a la couche de sortie Y de dimension n_outputs\n",
        "#     weights : la matrice des poids de dimension (n_inputs x n_outputs)\n",
        "#     inputs : le vecteur de sortie de la couche précedente de dimension n_inputs\n",
        "#     lrate : learning rate (scalar)\n",
        "#     \"\"\"\n",
        "#     dL_dW = []\n",
        "#     for xi in inputs:\n",
        "#         dL_dW = dL_dW + xi * dL_dY\n",
        "#     new_weights = flatten(weights) - lrate * dL_dW\n",
        "#     new_weights = new_weights.reshape(weights.shape)\n",
        "#     return new_weights\n",
        "\n",
        "# def calcul_dL_dX(dL_dY, weights):\n",
        "#     return np.dot(dL_dY, np.transpose(weights))\n",
        "\n",
        "\n",
        "# def fcl(inputs_layer, nbr_neurons, weights, biais, activation_type='ReLU'):\n",
        "#     current_layer = []\n",
        "#     current_layer = flatten(np.dot(inputs_layer, weights) + biais)[0]\n",
        "#     output_layer = activation_function(current_layer, activation_type)\n",
        "#     return output_layer\n",
        "   \n",
        "# def activation_function(layer, type_of_activation='relu'):\n",
        "#     type_of_activation = type_of_activation.lower()\n",
        "#     switcher = {\n",
        "#         'relu': ReLU,\n",
        "#         'tanh': tanh,\n",
        "#         'segmoid': segmoid\n",
        "#     }\n",
        "#     # Get the function from switcher dictionary\n",
        "#     activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' !\")\n",
        "#     return activation_type(layer)\n",
        "    \n",
        "    \n",
        "# def ReLU(layer):\n",
        "#     layer = np.array(layer)\n",
        "#     result = []\n",
        "#     for y in layer:\n",
        "#         result.append(np.max([y, 0]))\n",
        "#     return result\n",
        "\n",
        "# def tanh(layer):\n",
        "#     layer = np.array(layer)\n",
        "#     result = []\n",
        "#     for y in layer:\n",
        "#         r = (np.exp(y)-np.exp(-1*y))/(np.exp(y)+np.exp(-1*y))\n",
        "#         result.append(r)\n",
        "#     return result\n",
        "\n",
        "# def segmoid(layer):\n",
        "#     layer = np.array(layer)\n",
        "#     result = []\n",
        "#     for y in layer:\n",
        "#         r = 1/(1+np.exp(-1*y))\n",
        "#         result.append(r)\n",
        "#     return result\n",
        "\n",
        "def back_error_from_end_to_output_fcl(y_hat, y):\n",
        "    \"\"\"\n",
        "    arguments : y_hat = list des outputs calculés par le forward, et y = list des targets\n",
        "    cette fonction va calculer l'erreur de son origine(end_of_network) jusqu'à le output de fully_connected_layer\n",
        "    soit ce bout de network :\n",
        "    X ---> softmax(X) ---> CCE(y_hat, y) ---> Loss\n",
        "    alors cette fonction va retourner la dérivée de l'erreur Loss par rapport à X\n",
        "    càd elle return dL_dX\n",
        "    \"\"\"\n",
        "    return y_hat - y\n",
        "\n",
        "\n",
        "def unflatten(vector, pooled_layer):\n",
        "    pooled_layer = np.array(pooled_layer)\n",
        "    vector = np.array(vector)\n",
        "    # print(len(vector))\n",
        "    vector = vector.reshape(pooled_layer.shape)\n",
        "    return vector\n",
        "        \n",
        "        "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JGBU6bUeWwd"
      },
      "source": [
        "# Fully Connected Layer Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lin2XPzwsy7J"
      },
      "source": [
        "def activation_function(layer, type_of_activation='relu'):\n",
        "    type_of_activation = type_of_activation.lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' !\")\n",
        "    return activation_type(layer)\n",
        "    \n",
        "\n",
        "def ReLU(layer):\n",
        "    return layer * (layer > 0)\n",
        "\n",
        "\n",
        "def d_ReLU(layer):\n",
        "    return 1. * (layer > 0)\n",
        "\n",
        "\n",
        "def tanh(layer):\n",
        "    print('tanh')\n",
        "    r = (np.exp(layer)-np.exp(-1*layer))/(np.exp(layer)+np.exp(-1*layer))   \n",
        "    return np.array(r)\n",
        "\n",
        "\n",
        "def d_tanh(layer):\n",
        "    return 1 - tanh(layer) * tanh(layer)\n",
        "\n",
        "\n",
        "def segmoid(layer):\n",
        "    # print('segmoid')\n",
        "    return np.array(1/(1+np.exp(-1*layer)))\n",
        "\n",
        "\n",
        "def d_segmoid(vector):\n",
        "    \"\"\"\n",
        "    cette fontion prend un vector en entrée et retourne la dérivée de segmoid par rapport a ce vector\n",
        "    \"\"\"\n",
        "    return segmoid(vector) * (1 - segmoid(vector))\n",
        "\n",
        "\n",
        "# def softmax(data):\n",
        "#     # print('softmax')\n",
        "#     # print(data)\n",
        "#     proba_values = np.exp(data)/(np.sum(np.exp(data)))   \n",
        "#     return np.array(proba_values)\n",
        "\n",
        "def softmax(x):\n",
        "    maxi = np.max(x)\n",
        "    return np.exp(x-maxi)/sum(np.exp(x-maxi))\n",
        "\n",
        "\n",
        "def categoricalCrossEntropy(generated_values, target_values):\n",
        "    generated_values = [[1.0e-100] if r[0]==0.0 else r for r in generated_values]\n",
        "    somme = 0\n",
        "    for i in range(len(generated_values)):\n",
        "        somme = somme + target_values[i] * np.log(generated_values[i])\n",
        "    return (-1) * somme \n",
        "\n",
        "\n",
        "def normelize(img):\n",
        "    return img/255\n",
        "\n",
        "\n",
        "def flatten(img):\n",
        "    img = np.array(img) \n",
        "    return img.flatten()\n",
        "\n",
        "\n",
        "def show_image(img):\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def init_params(my_network):\n",
        "    nbr_layers = len(my_network) - 1\n",
        "    W, B = [], []\n",
        "    for i in range(nbr_layers):\n",
        "        W.append(np.random.randn(my_network[i+1], my_network[i]))\n",
        "        B.append(np.random.randn(my_network[i+1], 1))\n",
        "    return W, B\n",
        "\n",
        "\n",
        "def forward_pass(img, W, B):\n",
        "    \"\"\"\n",
        "    here we will use this notation :\n",
        "    Z[i] = W[i].X + B[i]\n",
        "    A[i] = activation_function(Z[i])\n",
        "    Z is a list that carries all the output of each layer\n",
        "    A is a list that carries all the output of each activation function\n",
        "    \"\"\"\n",
        "    act_functions = activation_functions_fcl[1:-1] # we omit the first element and the last one because the first activation will be None, and the last one will always be 'softmax'\n",
        "    act_functions = [type_of_activation.lower() for type_of_activation in act_functions] # lawercase all the items\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_types = [switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function\") for type_of_activation in act_functions]\n",
        "    Z, A = [], [img]\n",
        "    for i in range(len(W)):\n",
        "        if i == len(W)-1: # we have to use softmax as activation layer because we're in the last layer\n",
        "            Z.append(np.dot(W[i], A[i]) + B[i])\n",
        "            A.append(softmax(Z[i]))\n",
        "        else: # we're in hidden layer\n",
        "            Z.append(np.dot(W[i], A[i]) + B[i])\n",
        "            A.append(activation_types[i](Z[i]))\n",
        "    return Z, A\n",
        "\n",
        "\n",
        "def one_hot(y):\n",
        "    return np.eye(10)[y].reshape(10, 1)\n",
        "\n",
        "\n",
        "def update_W_and_B(W, dL_dW, B, dL_dB, lr):\n",
        "    \"\"\"\n",
        "    this function update the weights and Biais of myNetwork\n",
        "    arguments : \n",
        "    - W : it is a list that contains each Weight vector ([W1, W2, ...])\n",
        "    - dL_dW : derivatives of loss with respect to Weights (it is a list that contains Weights derivatives vectors [dL_dW1, dL_dW2, ...])\n",
        "    - B : it is a list that contains each Biais vector ([B1, B2, ...])\n",
        "    - dL_dB : derivatives of loss with respect to Biais (it is a list that contains Biais derivatives vectors [dL_dB1, dL_dB2, ...])\n",
        "    - lr : learning rate (real number)\n",
        "    \"\"\"\n",
        "    # print('Update_W_and_B')\n",
        "    new_W = []\n",
        "    new_B = []\n",
        "    \n",
        "    for w, dw in zip(W, dL_dW):\n",
        "        try:\n",
        "            w = np.array(w) - lr * np.array(dw)\n",
        "        except:\n",
        "            w = None\n",
        "        \n",
        "        new_W.append(w)\n",
        "    for b, db in zip(B, dL_dB):\n",
        "        try:\n",
        "            b = np.array(b) - lr * np.array(db)\n",
        "        except:\n",
        "            b = None\n",
        "        \n",
        "        new_B.append(b)\n",
        "        \n",
        "    return new_W, new_B\n",
        "\n",
        "\n",
        "def compute_accuracy(my_cnn, x_val, y_val, W, B):\n",
        "    '''\n",
        "        This function does a forward pass of x_validation, then checks if the indices\n",
        "        of the maximum value in the output equals the indices in the label\n",
        "        y. Then it sums over each prediction and calculates the accuracy.\n",
        "    '''\n",
        "    predictions = []\n",
        "\n",
        "    for x, y in zip(x_val, y_val):\n",
        "        # prepare the input image\n",
        "        # X = flatten(x)\n",
        "        # X = X.reshape(len(X), 1)\n",
        "        Y = one_hot(y)\n",
        "        # forward-propagation\n",
        "        Z, A = forward_propagation([x], my_cnn, W, B)\n",
        "        output = A[-1]\n",
        "        pred = np.argmax(output)\n",
        "        predictions.append(pred == np.argmax(Y))\n",
        "\n",
        "    return np.mean(predictions)\n",
        "\n",
        "\n",
        "def classify(img, W, B):\n",
        "    \"\"\"\n",
        "    cette fonction recois une seule image en parametre\n",
        "    et elle reçois les poids W et les Biais B et la liste des fonctions d'activations\n",
        "    et elle retourne la catégorie de l'image en entier 0..9\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    X = flatten(img)\n",
        "    X = X.reshape(len(X), 1)\n",
        "    # forward-propagation\n",
        "    Z, A = forward_pass(X, W, B)\n",
        "    output = A[-1]\n",
        "    pred = np.argmax(output)\n",
        "    return pred\n",
        "\n",
        "\n",
        "def show_accuracies(my_cnn, train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B):\n",
        "    \"\"\"\n",
        "    this function compute accuracy for each train-set, validation-set, and test-set\n",
        "    then print them all.\n",
        "    arguments : train_images, train_labels, val_images, val_labels, test_images, test_labels, W, B\n",
        "    \"\"\"\n",
        "    train_accuracy = compute_accuracy(my_cnn, train_images, train_labels, W, B)\n",
        "    val_accuracy = compute_accuracy(my_cnn, val_images, val_labels, W, B)\n",
        "    test_accuracy = compute_accuracy(my_cnn, test_images, test_labels, W, B)\n",
        "    print(\"Accuracies :\\n\\\n",
        "    - train accuracy = {} %\\n\\\n",
        "    - val accuracy = {} %\\n\\\n",
        "    - test accuracy = {} %\".format(train_accuracy*100, val_accuracy*100, test_accuracy*100))\n",
        "\n",
        "\n",
        "def backpro_pass(dL_dZ, A, Z, W, indice, indx_act_func):\n",
        "    # we omit the first element and the last one because the first activation will be None, and the last one will always be 'softmax'\n",
        "    act_functions = activation_functions[1:-1] \n",
        "    # lawercase all the items\n",
        "    act_functions = [type_of_activation.lower() for type_of_activation in act_functions] \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_types = [switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function\") for type_of_activation in act_functions]\n",
        "    \n",
        "    dl_dw = np.dot(dL_dZ, np.transpose(A[indice]))\n",
        "    dl_db = dL_dZ\n",
        "    dl_dz = 0\n",
        "    if indice*(-1) != len(Z)+1:\n",
        "      dl_da = np.dot(np.transpose(W[indice+1]), dL_dZ)\n",
        "      da_dz = activation_types[indx_act_func](Z[indice])\n",
        "      dl_dz = dl_da * da_dz\n",
        "    return dl_dw, dl_db, dl_dz"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx6wKd0xuzaj"
      },
      "source": [
        "# Here we will upload the dataset and normelize it then shuffle it then split it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQfMoEksy91"
      },
      "source": [
        "images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-images.idx3-ubyte'\n",
        "labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/train-labels.idx1-ubyte'\n",
        "test_images_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-images.idx3-ubyte'\n",
        "test_labels_path = '/content/drive/MyDrive/Colab Notebooks/mnist_data/test-labels.idx1-ubyte'\n",
        "test_images, test_labels = loadlocal_mnist(test_images_path, test_labels_path)\n",
        "train_images, train_labels = loadlocal_mnist(images_path, labels_path)\n",
        "\n",
        "# group all the images in one list\n",
        "# then normelize all the images\n",
        "images = np.concatenate([train_images, test_images])\n",
        "labels = np.concatenate([train_labels, test_labels])\n",
        "images = normelize(images)\n",
        "\n",
        "# shuffle all the images and all labels randomly\n",
        "random.seed(12)\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "labels = labels[indices]\n",
        "images = images[indices]\n",
        "\n",
        "# change shape of the images\n",
        "images = images.reshape(len(images), 28, 28)\n",
        "\n",
        "# split the data into train, validation and test \n",
        "train_images, val_images, test_images = images[:60000], images[60000:65000], images[65000:]\n",
        "train_labels, val_labels, test_labels = labels[:60000], labels[60000:65000], labels[65000:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3BP2WmGnLq7"
      },
      "source": [
        "# Learning  ------>  GO FOR LAUNCH !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFh0nVttXh36"
      },
      "source": [
        "def input_layer(dict):\n",
        "  dict['type_of_layer'] = 'input'\n",
        "  return dict\n",
        "\n",
        "def convolution_layer(dict):\n",
        "  dict['type_of_layer'] = 'convolution'\n",
        "  return dict\n",
        "\n",
        "def pooling_layer(dict):\n",
        "  dict['type_of_layer'] = 'pooling'\n",
        "  return dict\n",
        "\n",
        "def flatten_layer():\n",
        "  dict = {'type_of_layer': 'flatten'}\n",
        "  return dict\n",
        "\n",
        "def fcl(dict):\n",
        "  dict['type_of_layer'] = 'fcl'\n",
        "  return dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ltquYZzS18U",
        "outputId": "4aea8591-9d7c-4db1-db00-aef0b03534ac"
      },
      "source": [
        "\n",
        "my_cnn = [input_layer({\n",
        "              'width_image': 28,\n",
        "              'height_image': 28,\n",
        "              'nbr_channels': 1   # 1 --> means gray scale, and 3 --> means rgb\n",
        "              }\n",
        "          ),\n",
        "          convolution_layer({\n",
        "              'nbr_of_kernels':6, \n",
        "              'kernel_size':5, \n",
        "              'padding':0, \n",
        "              'stride':1, \n",
        "              'type_of_activation':'relu'\n",
        "              }\n",
        "          ), \n",
        "          pooling_layer({\n",
        "              'type_of_pooling' : 'MAX_POOLING',\n",
        "              'kernel_size' : 2,\n",
        "              'stride' : 2\n",
        "              } \n",
        "          ),\n",
        "          # convolution_layer({\n",
        "          #     'nbr_of_kernels':6, \n",
        "          #     'kernel_size':5, \n",
        "          #     'padding':0, \n",
        "          #     'stride':1, \n",
        "          #     'type_of_activation':'relu'\n",
        "          #     }\n",
        "          # ), \n",
        "          # pooling_layer({\n",
        "          #     'type_of_pooling' : 'MAX_POOLING',\n",
        "          #     'kernel_size' : 5,\n",
        "          #     'stride' : 2\n",
        "          #     } \n",
        "          # ),\n",
        "          flatten_layer(),\n",
        "        #   fcl({\n",
        "        #       'nbr_of_neurons' : 128, # 20 neurons in hidden layer\n",
        "        #       'type_of_activation' : 'segmoid', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "        #       'learning_rate' : 0.001\n",
        "        #         }\n",
        "        #   ),\n",
        "          fcl({\n",
        "              'nbr_of_neurons' : 10, # nbr of neurons in output_layer layer\n",
        "              'type_of_activation' : 'softmax', # 'tanh' will be the activation function in the hidden layer, and 'softmax' in the last layer\n",
        "              'learning_rate' : 0.001\n",
        "          }\n",
        "          )\n",
        "          ]\n",
        "my_cnn"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'height_image': 28,\n",
              "  'nbr_channels': 1,\n",
              "  'type_of_layer': 'input',\n",
              "  'width_image': 28},\n",
              " {'kernel_size': 5,\n",
              "  'nbr_of_kernels': 6,\n",
              "  'padding': 0,\n",
              "  'stride': 1,\n",
              "  'type_of_activation': 'relu',\n",
              "  'type_of_layer': 'convolution'},\n",
              " {'kernel_size': 2,\n",
              "  'stride': 2,\n",
              "  'type_of_layer': 'pooling',\n",
              "  'type_of_pooling': 'MAX_POOLING'},\n",
              " {'type_of_layer': 'flatten'},\n",
              " {'learning_rate': 0.001,\n",
              "  'nbr_of_neurons': 10,\n",
              "  'type_of_activation': 'softmax',\n",
              "  'type_of_layer': 'fcl'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTrefR2JcCJ9"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeKni3_lcCNO"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LcSNc8EcCXI"
      },
      "source": [
        "def forward_propagation(img, my_cnn, W, B):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'convolution': convolution_operation,\n",
        "        'pooling': pooling_operation,\n",
        "        'flatten': flatten_operation,\n",
        "        'fcl' : fcl_operation\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_activation_function\") for type_of_layer in my_cnn_architecture]\n",
        "    Z, A = [img], [img]\n",
        "    for i in range(1, len(my_cnn)):\n",
        "        z, a = operation_types[i](my_cnn[i], A, W, B, i)\n",
        "        Z.append(z)\n",
        "        A.append(a)\n",
        "    return Z, A"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA-ZtPajnUMc"
      },
      "source": [
        "# def input_operation(layer, A, W, B, layer_num):\n",
        "#     a = None\n",
        "#     z = None\n",
        "#     return a, z\n",
        "\n",
        "def convolution_operation(layer, A, W, B, layer_num):\n",
        "    previous_layer = A[-1]\n",
        "    nbr_filters = layer['nbr_of_kernels']\n",
        "    size_filter = layer['kernel_size']\n",
        "    convolved_layer = []\n",
        "    filters = W[layer_num]\n",
        "    biais = B[layer_num]\n",
        "    z = []\n",
        "    for i in range(len(filters)):\n",
        "        somme = 0\n",
        "        for feature_map in previous_layer:\n",
        "            somme = somme + convolution(feature_map, filters[i])\n",
        "        somme = somme + biais[i]\n",
        "        z.append(somme)\n",
        "        convolved_layer.append(ReLU(somme))\n",
        "    return z, convolved_layer\n",
        "\n",
        "def pooling_operation(layer, A, W, B, layer_num):\n",
        "    \"\"\"\n",
        "    convolved_layer : is a list that contains each convolved_map from previous_layer\n",
        "    type_of_pooling : should be either 'MAX_POOLING' or 'MEAN_POOLING' or 'MIN_POOLING'\n",
        "    size_of_pooling_kernel : is an integer that represents the shape of kernel \n",
        "                            (if size_of_pooling_kernel=2 then shape_kernel=(2, 2))\n",
        "    this function return a list that contains each pooled_map\n",
        "    \"\"\"\n",
        "    previous_layer = A[-1]\n",
        "    type_of_pooling = layer['type_of_pooling']\n",
        "    size_of_pooling_kernel = layer['kernel_size']\n",
        "    stride = layer['stride']\n",
        "    pooled_layer = []\n",
        "    switcher = {\n",
        "        'MAX_POOLING': max_pooling,\n",
        "        'MEAN_POOLING': mean_pooling,\n",
        "        'MIN_POOLING': min_pooling\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    pooling_operation = switcher.get(type_of_pooling, lambda: \"Invalid type_of_pooling !\")\n",
        "    # Execute the function\n",
        "    for map in previous_layer:\n",
        "        pooled_layer.append(pooling_operation(map, size_of_pooling_kernel, stride))\n",
        "    return pooled_layer, pooled_layer\n",
        "\n",
        "\n",
        "def flatten_operation(layer, A, W, B, layer_num):\n",
        "    a = flatten(A[-1])\n",
        "    a = a.reshape(len(a), 1)   \n",
        "    return a, a\n",
        "\n",
        "def fcl_operation(layer, A, Weights, B, layer_num):\n",
        "    # print('fcl_operation')\n",
        "    global W\n",
        "    global compteur\n",
        "    compteur += 1\n",
        "    input_fcl = A[-1]\n",
        "    if compteur == 1:\n",
        "        weights_fcl = np.random.randn(layer['nbr_of_neurons'], len(input_fcl))\n",
        "        W[layer_num] = weights_fcl\n",
        "    else:\n",
        "        weights_fcl = W[layer_num]\n",
        "    biais_fcl = B[layer_num]\n",
        "    type_of_activation = layer['type_of_activation'].lower()\n",
        "    switcher = {\n",
        "        'relu': ReLU,\n",
        "        'tanh': tanh,\n",
        "        'segmoid': segmoid,\n",
        "        'softmax': softmax\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    activation_type = switcher.get(type_of_activation, lambda: \"Invalid type_of_activation_function, please choose either 'ReLU' or 'tanh' or 'segmoid' or 'softmax' !\")\n",
        "    \n",
        "    # print(input_fcl)\n",
        "    # print(biais_fcl)\n",
        "    output_fcl = np.dot(weights_fcl, input_fcl) + biais_fcl\n",
        "    output = activation_type(output_fcl)\n",
        "    return output_fcl, output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0sy1r9Toqt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enj51gJnZ6fO"
      },
      "source": [
        "def input_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "def convolution_init_W_and_B(my_cnn, num_layer):\n",
        "    nbr_filters = my_cnn[num_layer]['nbr_of_kernels']\n",
        "    size_kernel = my_cnn[num_layer]['kernel_size']\n",
        "    w = [initialize_filter(size_kernel, size_kernel) for i in range(nbr_filters)]\n",
        "    b = initialize_filter(nbr_filters, 1)\n",
        "    return np.array(w), np.array(b)\n",
        "\n",
        "def pooling_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "\n",
        "def flatten_init_W_and_B(my_cnn, num_layer):\n",
        "    w, b = None, None\n",
        "    return w, b\n",
        "\n",
        "def fcl_init_W_and_B(my_cnn, num_layer):\n",
        "    global compteur_init\n",
        "    nbr_neurons = my_cnn[num_layer]['nbr_of_neurons']\n",
        "    if compteur_init == 0:\n",
        "        w = None\n",
        "        compteur_init += 1\n",
        "    else : \n",
        "        nbr_neurons_previous_layer = my_cnn[num_layer - 1]['nbr_of_neurons']\n",
        "        w = np.random.randn(nbr_neurons, nbr_neurons_previous_layer)\n",
        "    \n",
        "    b = initialize_filter(nbr_neurons, 1)\n",
        "    return w, np.array(b)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64fCBYvWYorg"
      },
      "source": [
        "def initialization(my_cnn):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "    switcher = {\n",
        "        'input': input_init_W_and_B,\n",
        "        'convolution': convolution_init_W_and_B,\n",
        "        'pooling': pooling_init_W_and_B,\n",
        "        'flatten': flatten_init_W_and_B,\n",
        "        'fcl' : fcl_init_W_and_B\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_activation_function\") for type_of_layer in my_cnn_architecture]\n",
        "    W, B = [], []\n",
        "    for i in range(len(my_cnn)):\n",
        "        w, b = operation_types[i](my_cnn, i)\n",
        "        # print(my_cnn[i])\n",
        "        # print(w)\n",
        "        W.append(w)\n",
        "        B.append(b)\n",
        "    \n",
        "    return np.array(W), np.array(B)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjTvf748dlgp"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HsM2r104MRj"
      },
      "source": [
        "def backpro_input(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print(\"backpro_input\")\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "def backpro_convolution(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print(\"backpro_convolution\")\n",
        "    dl_dz = d_ReLU(np.array(Z[num_layer])) * dL_dZ[-1]\n",
        "    dL_dF = []\n",
        "    dL_dX = []\n",
        "    \n",
        "    dL_dB = [np.sum(t) for t in dl_dz]\n",
        "    for map in A[num_layer - 1]:\n",
        "        X = np.array(map)\n",
        "        for dz in dl_dz:\n",
        "            dL_dF.append(convolution(X, dz))\n",
        "        # dl_dx = full_convolution(np.array(W[num_layer]), dL_dZ[-1])\n",
        "        # dL_dX.append(dl_dx)\n",
        "    # for dl_dz in dL_dZ[-1]:\n",
        "    #     dl_db = np.sum(dl_dz)\n",
        "    #     dL_dB.append(dl_db)\n",
        "    dL_dB = np.array(dL_dB)\n",
        "    dL_dB = dL_dB.reshape((dL_dB.shape[0], 1))\n",
        "    return dL_dF, dL_dB, dL_dX\n",
        "\n",
        "def backpro_pooling(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print(\"backpro_pooling\")\n",
        "    dl_dz = []\n",
        "    result = np.array(dL_dZ[-1])\n",
        "    stride = my_cnn[num_layer]['stride']\n",
        "    size_of_pooling_kernel = my_cnn[num_layer]['kernel_size']\n",
        "    for map, convolved_map in zip(result, Z[num_layer - 1]):\n",
        "        dl_dz_map = np.zeros_like(convolved_map)\n",
        "        for i in range(0, map.shape[0]):\n",
        "            for j in range(0, map.shape[1]):\n",
        "                value = map[i][j]\n",
        "                imaget = convolved_map[i*stride:i*stride+size_of_pooling_kernel, j*stride:j*stride+size_of_pooling_kernel]\n",
        "                imaget_max_index = np.argmax(imaget)\n",
        "                # imaget_max_value = np.max(imaget)\n",
        "                ligne, colonne = imaget_max_index//size_of_pooling_kernel , imaget_max_index%size_of_pooling_kernel\n",
        "                imaget = np.zeros_like(imaget)\n",
        "                imaget[ligne][colonne] = value\n",
        "                dl_dz_map[i*stride:i*stride+size_of_pooling_kernel, j*stride:j*stride+size_of_pooling_kernel] = imaget\n",
        "        dl_dz.append(np.array(dl_dz_map))\n",
        "\n",
        "    return None, None, np.array(dl_dz)\n",
        "\n",
        "def backpro_flatten(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print('backpro_flatten')\n",
        "    # print(type(dL_dZ[-1]))\n",
        "    # print(np.array(A[num_layer-1]).shape)\n",
        "    dl_dz = unflatten(np.array(dL_dZ[-1]), A[num_layer-1])\n",
        "    return None, None, dl_dz\n",
        "\n",
        "def backpro_fcl(my_cnn, dL_dZ, W, Z, A, num_layer):\n",
        "    # print('backpro_fcl')\n",
        "    type_of_activation = my_cnn[num_layer-1].get('type_of_activation')\n",
        "    \n",
        "    switcher = {\n",
        "        'relu': d_ReLU,\n",
        "        'tanh': d_tanh,\n",
        "        'segmoid': d_segmoid\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    if type_of_activation != None:\n",
        "        activation_type = switcher.get(type_of_activation, lambda: None)\n",
        "\n",
        "    # print(dL_dZ)\n",
        "    # print(num_layer)\n",
        "    dl_dw = np.dot(dL_dZ[-1], np.transpose(A[num_layer - 1]))\n",
        "    dl_db = dL_dZ[-1]\n",
        "    dl_da = np.dot(np.transpose(W[num_layer]), dL_dZ[-1])\n",
        "    \n",
        "    if type_of_activation == None:\n",
        "        dl_dz = dl_da\n",
        "    else:\n",
        "        da_dz = activation_type(np.array(Z[num_layer - 1]))\n",
        "        dl_dz = dl_da * da_dz\n",
        "    return np.array(dl_dw), np.array(dl_db), np.array(dl_dz)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_PuOYDcCeH"
      },
      "source": [
        "def backpropagation(my_cnn, dL_dZ, W, Z, A):\n",
        "    my_cnn_architecture = [my_cnn[layer]['type_of_layer'] for layer in range(len(my_cnn))]\n",
        "\n",
        "    switcher = {\n",
        "        'convolution': backpro_convolution,\n",
        "        'pooling': backpro_pooling,\n",
        "        'flatten': backpro_flatten,\n",
        "        'fcl' : backpro_fcl,\n",
        "        'input' : backpro_input\n",
        "    }\n",
        "    # Get the function from switcher dictionary\n",
        "    operation_types = [switcher.get(type_of_layer, lambda: \"Invalid type_of_activation_function\") for type_of_layer in my_cnn_architecture]\n",
        "    dL_dW, dL_dB = [], []\n",
        "    \n",
        "    for num_layer in range(len(my_cnn)-1, -1, -1): # iterate through all layers from output to input\n",
        "        dl_dw, dl_db, dl_dz = operation_types[num_layer](my_cnn, dL_dZ, W, Z, A, num_layer)\n",
        "        # print(num_layer, operation_types[num_layer], dl_dw)\n",
        "        dL_dW.append(dl_dw)\n",
        "        dL_dB.append(dl_db)\n",
        "        dL_dZ.append(dl_dz)\n",
        "    return dL_dW, dL_dB\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTHjzsuI9WXC"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "nofPg2oxcCZC",
        "outputId": "c6071ab7-1a71-4014-c92b-f04b7569ce0b"
      },
      "source": [
        "np.random.seed(50419)\n",
        "numbers_of_epochs = 250\n",
        "compteur_init = 0\n",
        "W, B = initialization(my_cnn)\n",
        "compteur = 0\n",
        "all_losses = []\n",
        "for epoch in range(numbers_of_epochs):\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    # for i in range(len(train_images)):\n",
        "    for i in range(1000):\n",
        "        # Z, A = [train_images[i]], [train_images[i]]\n",
        "        # Prepare the input image\n",
        "        # X = flatten(train_images[i])\n",
        "        # X = X.reshape(len(X), 1)\n",
        "        Y = one_hot(train_labels[i])\n",
        "        # if i==0 or i==5 or i==10 or i==15 or i==17 or i==18:\n",
        "        #     print(i, '-----> \\n',W)\n",
        "        # print('------------------------------------------------------------------------------------------')\n",
        "        Z, A = forward_propagation([train_images[i]], my_cnn, W, B)\n",
        "        loss = categoricalCrossEntropy(A[-1], Y)\n",
        "        # print(A[-1])\n",
        "        # print(i, ' ----> ', loss)\n",
        "        losses.append(loss)\n",
        "        # Backpropagation\n",
        "        dL_dZ2 = A[-1] - Y\n",
        "        dL_dZ = [dL_dZ2]\n",
        "        # here the variable indice has for aim to keep truck to which layer are we\n",
        "        # and the variable indx_act_func has the objectif to tell us which activation function should we use in each layer\n",
        "        indice, indx_act_func = 0, -1\n",
        "        # my_reverse_cnn = my_cnn[::-1] # reverse my_cnn and get a copy\n",
        "        dL_dW, dL_dB = backpropagation(my_cnn, dL_dZ, W, Z, A )\n",
        "        # if i >= 15:\n",
        "        #     print(Z[1])\n",
        "          \n",
        "        # update weights W and Biais B  \n",
        "        dL_dW.reverse()\n",
        "        dL_dB.reverse()\n",
        "        lr = my_cnn[-1]['learning_rate']\n",
        "        \n",
        "        W, B = update_W_and_B(W, dL_dW, B, dL_dB, lr)\n",
        "        \n",
        "\n",
        "\n",
        "        # dL_dW, dL_dB = backpropagation(Z, A, W)\n",
        "        # Update W and B\n",
        "        # W, B = update_weights_and_biais(W, dL_dW, B, dL_dB, 0.01)\n",
        "        # W, B = initialization(my_cnn)\n",
        "    \n",
        "    all_losses.append(mean(losses))\n",
        "    print(\"epoch num : \",epoch,\" loss : \",mean(losses), \" -------> time_epoch : \", time.time() - start_time, '------> accuracy = ',compute_accuracy(my_cnn, val_images[:1000], val_labels[:1000], W, B))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch num :  0  loss :  10.876157352829367  -------> time_epoch :  92.49486637115479 ------> accuracy =  0.119\n",
            "epoch num :  1  loss :  2.833790738473348  -------> time_epoch :  92.06016111373901 ------> accuracy =  0.112\n",
            "epoch num :  2  loss :  2.7184119916095155  -------> time_epoch :  91.83729124069214 ------> accuracy =  0.111\n",
            "epoch num :  3  loss :  2.647603438181738  -------> time_epoch :  92.41461539268494 ------> accuracy =  0.109\n",
            "epoch num :  4  loss :  2.592723846365375  -------> time_epoch :  92.11630082130432 ------> accuracy =  0.111\n",
            "epoch num :  5  loss :  2.5474858610908178  -------> time_epoch :  92.38790917396545 ------> accuracy =  0.113\n",
            "epoch num :  6  loss :  2.5093579480028767  -------> time_epoch :  92.06623816490173 ------> accuracy =  0.113\n",
            "epoch num :  7  loss :  2.476387013135044  -------> time_epoch :  92.35271334648132 ------> accuracy =  0.116\n",
            "epoch num :  8  loss :  2.447632851425934  -------> time_epoch :  92.31495308876038 ------> accuracy =  0.118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4f6f88f8da83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#     print(i, '-----> \\n',W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# print('------------------------------------------------------------------------------------------')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategoricalCrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# print(A[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-690a541f55e5>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(img, my_cnn, W, B)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_cnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-309d855bb46f>\u001b[0m in \u001b[0;36mpooling_operation\u001b[0;34m(layer, A, W, B, layer_num)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Execute the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprevious_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mpooled_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooling_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_of_pooling_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpooled_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-9e3f6d526555>\u001b[0m in \u001b[0;36mmax_pooling\u001b[0;34m(convolved_map, size_of_pooling_kernel, stride)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mimaget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolved_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize_of_pooling_kernel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize_of_pooling_kernel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimaget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m# print('------------------------')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# print(result.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m   2705\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2706\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSaRtAEGFrYe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxIRdobfFrbb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2EerGBOFreo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmrrYDnPFrhp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cf3Inu8FrkW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "D1I1lImi7uHY",
        "outputId": "cff24ba4-3ca3-4746-8a86-80d780ae642a"
      },
      "source": [
        "plt.plot(all_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3be660ee10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSqRYXeY7uEJ",
        "outputId": "eb46973a-f293-45f8-90c0-768be2460487"
      },
      "source": [
        "compute_accuracy(my_cnn, val_images_4, val_labels_4, W, B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxqeOyLO7uAx",
        "outputId": "b94db22f-5de3-4e15-f9ac-b80f59fa74d5"
      },
      "source": [
        "show_accuracies(my_cnn, images_4, labels_4, val_images_4, val_labels_4, test_images_4, test_labels_4, W, B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracies :\n",
            "    - train accuracy = 100.0 %\n",
            "    - val accuracy = 100.0 %\n",
            "    - test accuracy = 100.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XI0AxypD7t8_",
        "outputId": "437739fd-c949-48d5-a557-29eecd3998b1"
      },
      "source": [
        "indices = []\n",
        "for i in range(len(val_labels)):\n",
        "    if val_labels[i] == 4:\n",
        "        indices.append(i)\n",
        "\n",
        "val_labels_4 = val_labels[indices]\n",
        "val_images_4 = val_images[indices]\n",
        "show_multiple_images(val_images_4, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "FiGsiLeS7t44",
        "outputId": "ce240a2e-43b2-444e-fa1e-30f68f93d6f2"
      },
      "source": [
        "indices = []\n",
        "for i in range(len(test_labels)):\n",
        "    if test_labels[i] == 4:\n",
        "        indices.append(i)\n",
        "\n",
        "test_labels_4 = test_labels[indices]\n",
        "test_images_4 = test_images[indices]\n",
        "show_multiple_images(test_images_4, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBmLcMnX7tx1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3dNoCr_lbwS",
        "outputId": "6f108674-b006-4356-9834-caaa5f98f903"
      },
      "source": [
        "np.log(np.exp(np.array([700, 0, 1, 725, 750, 15])) * (-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, nan, nan, nan, nan, nan])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dpY5NRzBMoG",
        "outputId": "347c2243-c0c6-4c9f-bbf9-6de4e050ce91"
      },
      "source": [
        "dd = [None, None]\n",
        "if type(dd) != type(None):\n",
        "    print('hello')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOIAbN3vdQAf"
      },
      "source": [
        "[{'height_image': 28,\n",
        "  'nbr_channels': 1,\n",
        "  'type_of_layer': 'input',\n",
        "  'width_image': 28},\n",
        " {'kernel_size': 5,\n",
        "  'nbr_of_kernels': 6,\n",
        "  'padding': 0,\n",
        "  'stride': 1,\n",
        "  'type_of_activation': 'relu',\n",
        "  'type_of_layer': 'convolution'},\n",
        " {'kernel_size': 5,\n",
        "  'stride': 2,\n",
        "  'type_of_layer': 'pooling',\n",
        "  'type_of_pooling': 'MAX_POOLING'},\n",
        " {'type_of_layer': 'flatten'},\n",
        " {'learning_rate': 0.01,\n",
        "  'nbr_of_neurons': 20,\n",
        "  'type_of_activation': 'tanh',\n",
        "  'type_of_layer': 'fcl'},\n",
        " {'learning_rate': 0.01,\n",
        "  'nbr_of_neurons': 10,\n",
        "  'type_of_activation': 'softmax',\n",
        "  'type_of_layer': 'fcl'}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yHru48hdQEC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1P_-OyhdQHW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1b99U5NdQKn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rajvDin3dQNc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW8tyzhrdQQX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlKq_CeTdQTa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4VsQ2TzcCg0"
      },
      "source": [
        "# forward pass for 1000 images costs 137 seconds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZu76H9ScCj7"
      },
      "source": [
        "fi = [1, 2, 5]\n",
        "fi = fi[::-1]\n",
        "for i in fi:\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVx3rGJkcCme"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-R5hKErcCpM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}